import os
import json
import time
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
import matplotlib
import matplotlib.pyplot as plt
import warnings
from collections import deque, defaultdict
from datetime import datetime
from dataclasses import dataclass, asdict
import random
import matplotlib.dates as mdates
from matplotlib.ticker import PercentFormatter


os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
warnings.filterwarnings('ignore')

# ====================================
# ğŸ¯ EARLY STOPPING è¶…åƒæ•¸ (å¯èª¿å€)
# ====================================
EARLY_STOPPING_CONFIG = {
    'eval_interval': 10,      # æ¯ 10 å€‹ episode è©•ä¼°ä¸€æ¬¡
    'patience': 20,           # é€£çºŒ 20 æ¬¡æ²’æ”¹å–„å°±åœæ­¢
    'min_episodes': 30,       # è‡³å°‘è¨“ç·´ 30 å€‹ episode
    'use_sharpe_ratio': True, # True: ç”¨ Sharpe ratio, False: ç”¨å¹³å‡ reward
    'save_best_models': True  # æ˜¯å¦ä¿å­˜æœ€ä½³æ¨¡å‹
}

def setup_gpu():
    """GPUè¨­ç½®"""
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        try:
            gpu_to_use = gpus[3]
            tf.config.experimental.set_visible_devices(gpu_to_use, 'GPU')
            tf.config.experimental.set_memory_growth(gpu_to_use, True)
            print(f"âœ“ Using single GPU: {gpu_to_use.name}")
            return True
        except Exception as e:
            print(f"âœ— GPU setup failed: {e}, falling back to CPU")
            return False
    else:
        print("âœ— No GPU detected, using CPU")
        return False

def convert_numpy_types(obj):
    """éæ­¸è½‰æ›numpyé¡å‹ç‚ºPythonåŸç”Ÿé¡å‹"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    else:
        return obj

@dataclass
class EnhancedFinancialConfig:
    # â€”â€” ç›®æ¨™å æ¯”ï¼šSharpeâ‰ˆ45%ï¼ŒActiveâ‰ˆ45%ï¼ŒMDDâ‰ˆ8%ï¼ŒCashâ‰ˆ2% â€”â€”
    sharpe_weight: float = 0.45         # â†‘
    return_weight: float = 0.45         # â†‘
    mdd_weight: float = 0.08            # â†“
    cash_timing_weight: float = 0.02    # â†“

    # é¢¨éšªæ§åˆ¶åƒæ•¸
    target_max_drawdown: float = 0.15
    target_sharpe_threshold: float = 0.5

    # ç¾é‡‘ç®¡ç†åƒæ•¸
    cash_return_rate: float = 0.02
    optimal_cash_range: tuple = (0.05, 0.25)  # æ”¶çª„åˆ° 5%~25%

    # EMA åƒæ•¸
    alpha_fast: float = 0.08
    alpha_slow: float = 0.05

    # ç¸®æ”¾ä¿‚æ•¸ï¼ˆscaleï¼‰
    sharpe_scale: float = 6.0           # â†‘
    active_selection_scale: float = 20.0# â†‘
    mdd_penalty_scale: float = 2.0
    cash_timing_scale: float = 0.10     # â†“ å¤§å¹…é™

    
    def output_objectives(self):
        """ğŸ¯ Quant Model Layer è¼¸å‡ºï¼šé‡åŒ–æ¨¡å‹å®šç¾©çš„ç›®æ¨™"""
        return {
            "objectives": {
                "sharpe_weight": self.sharpe_weight,
                "return_weight": self.return_weight,
                "mdd_weight": self.mdd_weight,
                "cash_timing_weight": self.cash_timing_weight,
                "approach": "active_selection_fusion"
            },
            "risk_controls": {
                "target_max_drawdown": self.target_max_drawdown,
                "target_sharpe_threshold": self.target_sharpe_threshold
            },
            "cash_params": {
                "cash_return_rate": self.cash_return_rate,
                "optimal_cash_range": list(self.optimal_cash_range)
            },
            "scaling_factors": {
                "sharpe_scale": self.sharpe_scale,
                "active_selection_scale": self.active_selection_scale,
                "mdd_penalty_scale": self.mdd_penalty_scale,
                "cash_timing_scale": self.cash_timing_scale
            }
        }

class EnhancedFinancialRewardCalculator:
    """èåˆç‰ˆçå‹µè¨ˆç®—å™¨ï¼šSharpe + ActiveSelection + MDD + CashTiming"""
    
    def __init__(self, config: EnhancedFinancialConfig, n_stocks, lookback_window=20):
        self.config = config
        self.n_stocks = n_stocks
        self.n_assets = n_stocks + 1  # åŒ…æ‹¬ç¾é‡‘
        self.lookback_window = lookback_window
        
        # æ ¸å¿ƒè¿½è¹¤æ•¸æ“š
        self.portfolio_returns_history = deque(maxlen=lookback_window)
        self.benchmark_returns_history = deque(maxlen=lookback_window)
        self.weights_history = deque(maxlen=lookback_window)
        
        # EMAè¿½è¹¤
        self.portfolio_return_ema = 0.0
        self.portfolio_var_ema = 0.01
        self.benchmark_return_ema = 0.0
        self.excess_return_ema = 0.0
        self.excess_var_ema = 0.01
        
        # ğŸ†• MDD è¿½è¹¤è®Šæ•¸
        self.mdd_portfolio_values = []
        self.mdd_running_peak = 1.0  # å¾1.0é–‹å§‹ï¼Œä»£è¡¨åˆå§‹æ·¨å€¼
        
        # ç¾é‡‘ç›¸é—œ
        self.daily_cash_return = config.cash_return_rate / 252
        self.market_trend_ema = 0.0
        
        print("âœ“ Enhanced Financial Reward Calculator initialized (Fusion Version)")
        print(f"  Focus: Sharpe({config.sharpe_weight:.0%}) + ActiveSelection({config.return_weight:.0%}) + MDD({config.mdd_weight:.0%}) + Cash({config.cash_timing_weight:.0%})")
    
    def reset(self):
        """é‡ç½®è¨ˆç®—å™¨ç‹€æ…‹"""
        self.portfolio_returns_history.clear()
        self.benchmark_returns_history.clear()
        self.weights_history.clear()
        self.portfolio_return_ema = 0.0
        self.portfolio_var_ema = 0.01
        self.benchmark_return_ema = 0.0
        self.excess_return_ema = 0.0
        self.excess_var_ema = 0.01
        self.market_trend_ema = 0.0
        
        # é‡ç½®MDDè®Šæ•¸
        self.mdd_portfolio_values = []
        self.mdd_running_peak = 1.0
    
    def calculate_enhanced_reward(self, portfolio_value, portfolio_return, weights, benchmark_return, stock_returns, step):
        """è¨ˆç®—èåˆç‰ˆçå‹µ"""
        
        # æ›´æ–°MDDçš„æ­·å²æ•¸æ“š
        current_net_value = portfolio_value / 10000.0  # æ¨™æº–åŒ–æ·¨å€¼
        self.mdd_portfolio_values.append(current_net_value)
        self.mdd_running_peak = max(self.mdd_running_peak, current_net_value)
        
        # æ›´æ–°å…¶ä»–æ­·å²æ•¸æ“š
        self.portfolio_returns_history.append(portfolio_return)
        self.benchmark_returns_history.append(benchmark_return)
        self.weights_history.append(weights.copy())
        
        # æ›´æ–°EMA
        self._update_ema(portfolio_return, benchmark_return)
        
        # ğŸ¯ æ ¸å¿ƒçå‹µæˆåˆ†ï¼ˆèåˆç‰ˆï¼‰
        reward_components = {}
        
        # 1. Sharpe Ratioæœ€å¤§åŒ– (40%)
        reward_components['sharpe'] = self._calculate_sharpe_reward()
        
        # 2. å€‹è‚¡è¶…é¡å›å ±çå‹µ (30%) - èåˆç‰ˆæ ¸å¿ƒ
        reward_components['active_selection'] = self._calculate_active_selection_reward(weights, stock_returns)
        
        # 3. æœ€å¤§å›æ’¤æ‡²ç½° (15%)
        reward_components['mdd_penalty'] = self._calculate_mdd_penalty()
        
        # 4. ç¾é‡‘æ™‚æ©Ÿé¸æ“‡ (15%)
        reward_components['cash_timing'] = self._calculate_cash_timing_reward(weights[-1], stock_returns)
        
        # çµ„åˆæœ€çµ‚çå‹µ
        final_reward = self._combine_enhanced_rewards(reward_components)
        
        return final_reward, reward_components
    
    def _calculate_sharpe_reward(self):
        if len(self.portfolio_returns_history) < 10:
            return 0.0
        std = np.sqrt(max(self.portfolio_var_ema - self.portfolio_return_ema**2, 1e-6))
        excess = self.portfolio_return_ema - self.daily_cash_return
        sharpe = excess / std
        sharpe = np.clip(sharpe, -3.0, 3.0)  # æŠ‘åˆ¶æ¥µç«¯
        bonus = max(0.0, (sharpe - self.config.target_sharpe_threshold) * 2.0)
        return (sharpe + bonus) * self.config.sharpe_scale

    
    def _calculate_active_selection_reward(self, weights, stock_returns):
        market_avg = np.mean(stock_returns)
        excess = stock_returns - market_avg
        cs_std = np.std(excess)
        cs_std = max(cs_std, 1e-5)

        stock_weights = weights[:-1]  # exclude cash
        active_signal = np.dot(stock_weights, excess) / cs_std   # æ¨™æº–åŒ–å¾Œ
        return active_signal * self.config.active_selection_scale

    
    def _calculate_mdd_penalty(self):
        """è¨ˆç®—æœ€å¤§å›æ’¤æ‡²ç½°"""
        if not self.mdd_portfolio_values:
            return 0.0

        # è¨ˆç®—ç•¶å‰å›æ’¤ç™¾åˆ†æ¯”ï¼ˆæœƒæ˜¯0æˆ–è² æ•¸ï¼‰
        current_drawdown = (self.mdd_portfolio_values[-1] - self.mdd_running_peak) / self.mdd_running_peak
        
        # å›æ’¤è¶Šå¤§ï¼Œæ‡²ç½°è¶Šå¤§
        penalty = current_drawdown * self.config.mdd_penalty_scale
        
        return penalty
    
    def _calculate_cash_timing_reward(self, cash_ratio, stock_returns):
        mkt = float(np.mean(stock_returns))
        self.market_trend_ema = (1-self.config.alpha_fast) * self.market_trend_ema + self.config.alpha_fast * mkt

        # æ­»å€ï¼šå°æ³¢å‹•ä¸çµ¦åˆ†
        dead_zone = 0.01
        extreme = 0.03

        if mkt <= -extreme:         # æ€¥è·Œâ†’å¤šç¾é‡‘å¥½
            timing_score = cash_ratio
        elif mkt >=  extreme:       # æ€¥æ¼²â†’å°‘ç¾é‡‘å¥½
            timing_score = 1.0 - cash_ratio
        elif abs(mkt) <= dead_zone: # å°æ³¢å‹•â†’ä¸çä¸ç½°
            timing_score = 0.0
        else:
            # ç·©å’Œå€ï¼šç·šæ€§æ’å€¼ï¼Œé¿å…å¤§å¹…åº¦
            frac = (abs(mkt)-dead_zone)/(extreme-dead_zone)
            if mkt > 0:
                timing_score = (1.0 - cash_ratio) * frac
            else:
                timing_score = cash_ratio * frac

        # é™å¹…ï¼Œç¢ºä¿ç¾é‡‘é …ä¸ä¸»å®°
        timing_score = np.clip(timing_score, -0.5, 0.5)
        return timing_score * self.config.cash_timing_scale

    
    def _update_ema(self, portfolio_return, benchmark_return):
        """æ›´æ–°EMAè¿½è¹¤æŒ‡æ¨™"""
        alpha_fast = self.config.alpha_fast
        alpha_slow = self.config.alpha_slow
        
        # æŠ•è³‡çµ„åˆEMA
        self.portfolio_return_ema = (1-alpha_slow) * self.portfolio_return_ema + alpha_slow * portfolio_return
        self.portfolio_var_ema = (1-alpha_slow) * self.portfolio_var_ema + alpha_slow * portfolio_return**2
        
        # åŸºæº–EMA
        self.benchmark_return_ema = (1-alpha_slow) * self.benchmark_return_ema + alpha_slow * benchmark_return
        
        # è¶…é¡æ”¶ç›ŠEMA
        excess_return = portfolio_return - benchmark_return
        self.excess_return_ema = (1-alpha_slow) * self.excess_return_ema + alpha_slow * excess_return
        self.excess_var_ema = (1-alpha_slow) * self.excess_var_ema + alpha_slow * excess_return**2
    
    def _combine_enhanced_rewards(self, reward_components):
        """çµ„åˆèåˆç‰ˆçå‹µ"""
        weights = {
            'sharpe': self.config.sharpe_weight,
            'active_selection': self.config.return_weight,
            'mdd_penalty': self.config.mdd_weight,
            'cash_timing': self.config.cash_timing_weight,
        }
        
        total_reward = sum(reward_components[key] * weights[key] for key in reward_components.keys())
        return total_reward

class ReplayBuffer:
    """ç¶“é©—å›æ”¾ç·©è¡å€ for TD3"""
    def __init__(self, capacity=100000):
        self.capacity = capacity
        self.buffer = []
        self.position = 0
    
    def push(self, state, action, reward, next_state, done):
        if len(self.buffer) < self.capacity:
            self.buffer.append(None)
        self.buffer[self.position] = (state, action, reward, next_state, done)
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = map(np.stack, zip(*batch))
        return states, actions, rewards, next_states, dones
    
    def __len__(self):
        return len(self.buffer)

class TD3Agent:
    """ğŸ†• Twin Delayed Deep Deterministic Policy Gradient (TD3) Agent with Early Stopping"""
    def __init__(self, state_dim, action_dim, learning_rate=3e-4):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.learning_rate = learning_rate
        
        # TD3 è¶…åƒæ•¸
        self.gamma = 0.99
        self.tau = 0.005                    # è»Ÿæ›´æ–°ä¿‚æ•¸
        self.batch_size = 256
        self.policy_delay = 2               # å»¶é²ç­–ç•¥æ›´æ–°
        self.policy_noise = 0.05             # ç›®æ¨™ç­–ç•¥å¹³æ»‘å™ªè²
        self.noise_clip = 0.10               # å™ªè²è£å‰ª
        self.exploration_noise = 0.03        # æ¢ç´¢å™ªè²
        self.max_grad_norm = 1.0            # æ¢¯åº¦è£å‰ª
        
        # è¨“ç·´è¨ˆæ•¸å™¨
        self.training_step = 0
        
        # ğŸ†• æ§‹å»ºTD3ç¶²çµ¡æ¶æ§‹
        self.actor = self._build_actor()                    # ç¢ºå®šæ€§ç­–ç•¥
        self.actor_target = self._build_actor()             # ç›®æ¨™ç­–ç•¥
        self.critic1 = self._build_critic()                 # Twin Critic 1
        self.critic2 = self._build_critic()                 # Twin Critic 2
        self.critic1_target = self._build_critic()          # ç›®æ¨™ Critic 1
        self.critic2_target = self._build_critic()          # ç›®æ¨™ Critic 2
        
        # åˆå§‹åŒ–ç›®æ¨™ç¶²çµ¡æ¬Šé‡
        self.actor_target.set_weights(self.actor.get_weights())
        self.critic1_target.set_weights(self.critic1.get_weights())
        self.critic2_target.set_weights(self.critic2.get_weights())
        
        # å„ªåŒ–å™¨
        self.actor_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
        self.critic1_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
        self.critic2_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
        
        # ç¶“é©—å›æ”¾
        self.replay_buffer = ReplayBuffer()
        
        print("âœ“ TD3 Agent initialized (Twin Delayed DDPG with Early Stopping)")
        print(f"  State dim: {state_dim}, Action dim: {action_dim}")
        print(f"  ğŸ†• Networks: Actor + Twin Critics + 3Ã—Target Networks")
        print(f"  ğŸ¯ TD3 Features: Policy Delay + Target Smoothing + Twin Critics")
        print(f"  âŒ No entropy term (deterministic policy)")
        print(f"  ğŸ”„ Early Stopping Support: Model save/load enabled")
    
    def _build_actor(self):
        """æ§‹å»ºç¢ºå®šæ€§Actorç¶²çµ¡"""
        inputs = keras.Input(shape=(self.state_dim,))
        
        x = keras.layers.Dense(512, activation='relu')(inputs)
        x = keras.layers.Dense(256, activation='relu')(x)
        x = keras.layers.Dense(128, activation='relu')(x)
        
        # TD3: ç¢ºå®šæ€§è¼¸å‡ºï¼Œä½¿ç”¨tanhæ¿€æ´»
        actions = keras.layers.Dense(self.action_dim, activation='tanh')(x)
        
        model = keras.Model(inputs=inputs, outputs=actions)
        return model
    
    def _build_critic(self):
        """æ§‹å»ºCriticç¶²çµ¡ - Q(s,a)"""
        state_input = keras.Input(shape=(self.state_dim,))
        action_input = keras.Input(shape=(self.action_dim,))
        
        concat = keras.layers.Concatenate()([state_input, action_input])
        
        x = keras.layers.Dense(512, activation='relu')(concat)
        x = keras.layers.Dense(256, activation='relu')(x)
        x = keras.layers.Dense(128, activation='relu')(x)
        q_value = keras.layers.Dense(1, activation='linear')(x)
        
        model = keras.Model(inputs=[state_input, action_input], outputs=q_value)
        return model
    
    def get_action(self, state, add_noise=True):
        """ç²å–ç¢ºå®šæ€§å‹•ä½œï¼ˆå¯é¸æ¢ç´¢å™ªè²ï¼‰"""
        if len(state.shape) == 1:
            state = tf.expand_dims(state, 0)
        
        action = self.actor(state)[0]
        
        # æ¢ç´¢å™ªè²ï¼ˆè¨“ç·´æ™‚ï¼‰
        if add_noise:
            noise = tf.random.normal(shape=tf.shape(action), stddev=self.exploration_noise)
            action = action + noise
            action = tf.clip_by_value(action, -1.0, 1.0)
        
        action_info = {
            "state": state[0].numpy().tolist(),
            "action": action.numpy().tolist(),
            "add_noise": add_noise,
            "exploration_noise": self.exploration_noise
        }
        
        return action.numpy(), action_info
    
    def add_experience(self, state, action, reward, next_state, done):
        self.replay_buffer.push(state, action, reward, next_state, done)
    
    def soft_update(self, target_model, source_model):
        """è»Ÿæ›´æ–°ç›®æ¨™ç¶²çµ¡"""
        target_weights = target_model.get_weights()
        source_weights = source_model.get_weights()
        
        for i in range(len(target_weights)):
            target_weights[i] = self.tau * source_weights[i] + (1 - self.tau) * target_weights[i]
        
        target_model.set_weights(target_weights)
    
    def save_best_models(self, save_dir):
        """ğŸ†• ä¿å­˜æœ€ä½³æ¨¡å‹æ¬Šé‡"""
        try:
            os.makedirs(save_dir, exist_ok=True)
            self.actor.save(f'{save_dir}/best_actor.keras')
            self.critic1.save(f'{save_dir}/best_critic1.keras')
            self.critic2.save(f'{save_dir}/best_critic2.keras')
            print(f"âœ… Best models saved to {save_dir}")
        except Exception as e:
            print(f"âš ï¸ Failed to save best models: {e}")
    
    def load_best_models(self, save_dir):
        """ğŸ†• è¼‰å…¥æœ€ä½³æ¨¡å‹æ¬Šé‡"""
        try:
            if os.path.exists(f'{save_dir}/best_actor.keras'):
                self.actor = keras.models.load_model(f'{save_dir}/best_actor.keras')
                print(f"âœ… Best actor loaded from {save_dir}")
            if os.path.exists(f'{save_dir}/best_critic1.keras'):
                self.critic1 = keras.models.load_model(f'{save_dir}/best_critic1.keras')
                print(f"âœ… Best critic1 loaded from {save_dir}")
            if os.path.exists(f'{save_dir}/best_critic2.keras'):
                self.critic2 = keras.models.load_model(f'{save_dir}/best_critic2.keras')
                print(f"âœ… Best critic2 loaded from {save_dir}")
        except Exception as e:
            print(f"âš ï¸ Failed to load best models: {e}")
    
    def train(self):
        """ğŸ†• TD3 è¨“ç·´æµç¨‹"""
        if len(self.replay_buffer) < self.batch_size:
            return {}
        
        try:
            # å¾å›æ”¾ç·©è¡å€æ¡æ¨£
            states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)
            
            states = tf.convert_to_tensor(states, dtype=tf.float32)
            actions = tf.convert_to_tensor(actions, dtype=tf.float32)
            rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)
            next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)
            dones = tf.convert_to_tensor(dones, dtype=tf.float32)
            
            # æª¢æŸ¥æ•¸æ“šæœ‰æ•ˆæ€§
            if (tf.reduce_any(tf.math.is_nan(states)) or tf.reduce_any(tf.math.is_nan(actions)) or 
                tf.reduce_any(tf.math.is_nan(rewards)) or tf.reduce_any(tf.math.is_nan(next_states))):
                return {'actor_loss': 0.0, 'critic1_loss': 0.0, 'critic2_loss': 0.0}
            
            rewards = tf.clip_by_value(rewards, -10.0, 10.0)
            
            # ğŸ¯ ç¬¬ä¸€æ­¥ï¼šè¨“ç·´Criticsï¼ˆæ¯æ­¥éƒ½è¨“ç·´ï¼‰
            critic1_loss, critic2_loss = self._train_critics(states, actions, rewards, next_states, dones)
            
            actor_loss = None
            # ğŸ¯ ç¬¬äºŒæ­¥ï¼šå»¶é²è¨“ç·´Actorï¼ˆæ¯policy_delayæ­¥è¨“ç·´ä¸€æ¬¡ï¼‰
            if self.training_step % self.policy_delay == 0:
                actor_loss = self._train_actor(states)
                # è»Ÿæ›´æ–°æ‰€æœ‰ç›®æ¨™ç¶²çµ¡
                self.soft_update(self.actor_target, self.actor)
                self.soft_update(self.critic1_target, self.critic1)
                self.soft_update(self.critic2_target, self.critic2)
            
            self.training_step += 1
            
            return {
                'actor_loss': float(actor_loss) if actor_loss is not None else 0.0,
                'critic1_loss': float(critic1_loss),
                'critic2_loss': float(critic2_loss),
                'avg_q1_value': float(tf.reduce_mean(self.critic1([states, actions]))),
                'avg_q2_value': float(tf.reduce_mean(self.critic2([states, actions]))),
                'training_step': self.training_step
            }
            
        except Exception as e:
            print(f"âš ï¸ TD3 Training error: {e}")
            return {'actor_loss': 0.0, 'critic1_loss': 0.0, 'critic2_loss': 0.0}
    
    def _train_critics(self, states, actions, rewards, next_states, dones):
        """è¨“ç·´Twin Critics"""
        with tf.GradientTape(persistent=True) as tape:
            # ğŸ¯ TD3 ç›®æ¨™ç­–ç•¥å¹³æ»‘ (Target Policy Smoothing)
            target_actions = self.actor_target(next_states)
            
            # æ·»åŠ è£å‰ªå™ªè²åˆ°ç›®æ¨™å‹•ä½œ
            noise = tf.random.normal(shape=tf.shape(target_actions), stddev=self.policy_noise)
            noise = tf.clip_by_value(noise, -self.noise_clip, self.noise_clip)
            target_actions = tf.clip_by_value(target_actions + noise, -1.0, 1.0)
            
            # ğŸ¯ TD3 é›™å»¶é²Qå­¸ç¿’ (Clipped Double Q-Learning)
            target_q1 = self.critic1_target([next_states, target_actions])
            target_q2 = self.critic2_target([next_states, target_actions])
            target_q = tf.minimum(target_q1, target_q2)  # å–æœ€å°å€¼æ¸›å°‘éä¼°è¨ˆ
            target_q = rewards + self.gamma * (1 - dones) * tf.squeeze(target_q)
            target_q = tf.clip_by_value(target_q, -50.0, 50.0)
            
            # ç•¶å‰Qå€¼
            current_q1 = tf.squeeze(self.critic1([states, actions]))
            current_q2 = tf.squeeze(self.critic2([states, actions]))
            
            # Criticsæå¤±
            critic1_loss = tf.reduce_mean((current_q1 - target_q) ** 2)
            critic2_loss = tf.reduce_mean((current_q2 - target_q) ** 2)
        
        # æ›´æ–°Critic1
        critic1_grads = tape.gradient(critic1_loss, self.critic1.trainable_variables)
        if critic1_grads:
            critic1_grads = [tf.clip_by_norm(g, self.max_grad_norm) for g in critic1_grads if g is not None]
            self.critic1_optimizer.apply_gradients(zip(critic1_grads, self.critic1.trainable_variables))
        
        # æ›´æ–°Critic2
        critic2_grads = tape.gradient(critic2_loss, self.critic2.trainable_variables)
        if critic2_grads:
            critic2_grads = [tf.clip_by_norm(g, self.max_grad_norm) for g in critic2_grads if g is not None]
            self.critic2_optimizer.apply_gradients(zip(critic2_grads, self.critic2.trainable_variables))
        
        del tape
        return critic1_loss, critic2_loss
    
    def _train_actor(self, states):
        """è¨“ç·´Actorï¼ˆå»¶é²æ›´æ–°ï¼‰"""
        with tf.GradientTape() as tape:
            # ä½¿ç”¨Critic1è¨ˆç®—ç­–ç•¥æ¢¯åº¦ï¼ˆTD3åªç”¨å…¶ä¸­ä¸€å€‹Criticï¼‰
            actions = self.actor(states)
            actor_loss = -tf.reduce_mean(self.critic1([states, actions]))
        
        # æ›´æ–°Actor
        if not tf.math.is_nan(actor_loss):
            actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)
            if actor_grads:
                actor_grads = [tf.clip_by_norm(g, self.max_grad_norm) for g in actor_grads if g is not None]
                self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))
        
        return actor_loss

def evaluate_on_validation(agent, val_env):
    """
    ğŸ†• åœ¨é©—è­‰é›†ä¸Šè©•ä¼°æ¨¡å‹æ€§èƒ½
    è¿”å› Sharpe ratio æˆ–å¹³å‡ reward (æ ¹æ“š EARLY_STOPPING_CONFIG æ±ºå®š)
    """
    state = val_env.reset()
    rewards = []
    done = False
    
    while not done:
        # é©—è­‰æ™‚ä¸åŠ å™ªè²
        action, _ = agent.get_action(state, add_noise=False)
        state, reward, done, _ = val_env.step(action)
        rewards.append(reward)
    
    rewards = np.array(rewards)
    
    if EARLY_STOPPING_CONFIG['use_sharpe_ratio']:
        # è¨ˆç®— Sharpe ratio
        if len(rewards) > 1:
            mean_return = np.mean(rewards)
            std_return = np.std(rewards) + 1e-8  # é¿å…é™¤é›¶
            sharpe_ratio = mean_return / std_return * np.sqrt(252)  # å¹´åŒ– Sharpe
            return sharpe_ratio
        else:
            return 0.0
    else:
        # è¿”å›å¹³å‡ reward
        return np.mean(rewards)

class EnhancedPortfolioEnvironment:
    """å¢å¼·ç‰ˆæŠ•è³‡çµ„åˆç’°å¢ƒ - è±å¯Œç‹€æ…‹ç‰¹å¾µ + èåˆç‰ˆçå‹µå‡½æ•¸"""
    
    def __init__(self, csv_file_path, mode='train', config=None):
        self.csv_file_path = csv_file_path
        self.mode = mode
        
        # è¼‰å…¥ä¸¦é è™•ç†æ•¸æ“š
        print(f"ğŸ“Š Loading and preprocessing data for enhanced features...")
        self._load_and_preprocess_enhanced()
        
        # è¨­ç½®é…ç½®
        if config is None:
            config = EnhancedFinancialConfig()
        self.config = config
        
        # è¨­ç½®ç¾é‡‘æ”¶ç›Šç‡
        self.daily_cash_return = config.cash_return_rate / 252
        
        # ç’°å¢ƒç‹€æ…‹
        self.current_step = 0
        self.portfolio_value = 10000.0
        
        # å‹•æ…‹åˆå§‹æ¬Šé‡ï¼šè‚¡ç¥¨ç­‰æ¬Šé‡80%ï¼Œç¾é‡‘20%
        stock_weight = 0.8 / self.n_stocks
        self.initial_weights = np.array([stock_weight] * self.n_stocks + [0.2], dtype=np.float32)
        
        self.weights = self.initial_weights.copy()
        self.weights_history = []
        self.portfolio_values_history = []
        
        # èåˆç‰ˆçå‹µè¨ˆç®—å™¨
        self.reward_calculator = EnhancedFinancialRewardCalculator(
            config=config,
            n_stocks=self.n_stocks,
            lookback_window=20
        )
        
        # è¿½è¹¤è©³ç´°çå‹µä¿¡æ¯
        self.reward_history = []
        
        print(f"âœ“ Enhanced Portfolio Environment initialized (Fusion Version)")
        print(f"âœ“ State dim: {self.state_dim}, Action dim: {self.action_dim}")
        print(f"âœ“ Features per stock: 5 (mom5d, mom20d, vol20d, rsi, bollinger_b)")
        print(f"âœ“ Total trading days: {self.n_days}")
    
    def _load_and_preprocess_enhanced(self):
        """è¼‰å…¥ä¸¦é è™•ç†æ•¸æ“š - å¢åŠ è±å¯Œçš„æŠ€è¡“ç‰¹å¾µ"""
        
        # 1. è¼‰å…¥åŸå§‹æ•¸æ“š
        df = pd.read_csv(self.csv_file_path)
        df['Date'] = pd.to_datetime(df['Date'])
        df.set_index('Date', inplace=True)
        df = df.ffill().dropna()
        
        print(f"âœ“ Loaded price data: {df.shape}")
        
        # 2. è¨ˆç®—å›å ±ç‡
        returns = df.pct_change().fillna(0)
        
        # 3. è¨ˆç®—è±å¯Œçš„æŠ€è¡“ç‰¹å¾µ
        print(f"ğŸ“ˆ Computing enhanced technical features...")
        
        # æ ¸å¿ƒæ“´å±•ï¼šå‹•èƒ½å’Œæ³¢å‹•æ€§
        momentum_5d = returns.rolling(window=5).mean().fillna(0)
        momentum_20d = returns.rolling(window=20).mean().fillna(0)
        volatility_20d = returns.rolling(window=20).std().fillna(0)
        
        # æŠ€è¡“æŒ‡æ¨™ï¼šRSI
        delta = returns.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / (loss + 1e-8)
        rsi = 100 - (100 / (1 + rs))
        rsi = rsi.fillna(50) / 100.0  # æ¨™æº–åŒ–åˆ° [0,1]
        
        # æŠ€è¡“æŒ‡æ¨™ï¼šå¸ƒæ—å¸¶ %B
        ma_20 = df.rolling(window=20).mean()
        std_20 = df.rolling(window=20).std()
        upper_band = ma_20 + (2 * std_20)
        lower_band = ma_20 - (2 * std_20)
        bollinger_b = (df - lower_band) / (upper_band - lower_band + 1e-8)
        bollinger_b = bollinger_b.fillna(0.5)  # ç”¨0.5å¡«å……ï¼ˆä¸­ä½å€¼ï¼‰
        
        # 4. åˆä½µæ‰€æœ‰ç‰¹å¾µ
        self.feature_df = pd.concat([
            momentum_5d.add_suffix('_mom5d'),
            momentum_20d.add_suffix('_mom20d'),
            volatility_20d.add_suffix('_vol20d'),
            rsi.add_suffix('_rsi'),
            bollinger_b.add_suffix('_bollinger_b'),
        ], axis=1)
            
        # --- é—œéµä¿®æ­£ï¼šå°‡æ‰€æœ‰ç‰¹å¾µæ•¸æ“šå‘ä¸‹å¹³ç§»ä¸€å¤© ---
        # é€™æ¨£åœ¨ t æ—¥åˆ»åšæ±ºç­–æ™‚ï¼Œæˆ‘å€‘ç”¨çš„æ˜¯ t-1 æ—¥çš„ç‰¹å¾µ
        self.feature_df = self.feature_df#.shift(1)
        
        # å¹³ç§»å¾Œç¬¬ä¸€è¡Œæœƒæ˜¯ NaNï¼Œéœ€è¦è™•ç†
        # æˆ‘å€‘åŒæ™‚ä¹Ÿå¹³ç§»å›å ±ç‡æ•¸æ“šï¼Œä»¥ç¢ºä¿å°é½Š
        self.price_data = df.copy()
        self.returns_data = returns.copy().shift(-1)

        # é‡æ–°å°é½Šä¸¦å»é™¤å› å¹³ç§»ç”¢ç”Ÿçš„NaNå€¼
        combined = pd.concat([self.feature_df, self.returns_data], axis=1).dropna()

        self.feature_df = combined[self.feature_df.columns]
        self.returns_data = combined[self.returns_data.columns]

        print(f"âœ… Features shifted to prevent lookahead bias.")
     
        # 6. ç²å–è‚¡ç¥¨æ•¸é‡å’Œåç¨±
        self.n_stocks = df.shape[1]
        self.stock_names = df.columns.tolist()
        self.n_assets = self.n_stocks + 1
        
        print(f"âœ“ Enhanced features computed: {self.feature_df.shape}")
        print(f"âœ“ Stocks detected: {self.n_stocks}")
        
        # 7. æ•¸æ“šé›†åˆ†å‰²
        total = len(self.feature_df)
        train_end = int(0.7 * total)
        val_end = int(0.8 * total)
        
        if self.mode == 'train':
            self.features = self.feature_df.iloc[:train_end].values
            self.returns = self.returns_data.iloc[:train_end].values
            self.price_subset = df.iloc[:train_end]
        elif self.mode == 'val':
            self.features = self.feature_df.iloc[train_end:val_end].values
            self.returns = self.returns_data.iloc[train_end:val_end].values
            self.price_subset = df.iloc[train_end:val_end]
        else:  # test
            self.features = self.feature_df.iloc[val_end:].values
            self.returns = self.returns_data.iloc[val_end:].values
            self.price_subset = df.iloc[val_end:]
        
        self.n_days = len(self.features)
        
        # 8. é‡æ–°è¨ˆç®—state_dim
        # æ¯å€‹è‚¡ç¥¨æœ‰5å€‹ç‰¹å¾µ + n_assetså€‹ç•¶å‰æ¬Šé‡ + 1å€‹ç¾é‡‘å›å ±ç‡
        self.state_dim = (self.n_stocks * 5) + self.n_assets + 1
        self.action_dim = self.n_assets
        
        print(f"âœ“ Data split completed: {self.n_days} days for {self.mode}")
        print(f"âœ“ State dimension: {self.state_dim}")
    
    def reset(self):
        """é‡ç½®ç’°å¢ƒ"""
        self.current_step = 0
        self.portfolio_value = 10000.0
        self.weights = self.initial_weights.copy()
        self.weights_history = []
        self.portfolio_values_history = [self.portfolio_value]
        self.reward_history = []
        
        # é‡ç½®çå‹µè¨ˆç®—å™¨
        self.reward_calculator.reset()
        
        return self._get_current_state()
    
    def _get_current_state(self):
        """ç²å–ç•¶å‰çš„å¢å¼·ç‰ˆç‹€æ…‹"""
        if self.current_step >= len(self.features):
            return np.zeros(self.state_dim, dtype=np.float32)
        
        # 1. è‚¡ç¥¨æŠ€è¡“ç‰¹å¾µ (n_stocks * 5)
        stock_features = self.features[self.current_step]
        
        # 2. ç¾é‡‘ç‰¹å¾µ (1)
        cash_feature = np.array([self.daily_cash_return])
        
        # 3. ç•¶å‰æŒå€‰æ¬Šé‡ (n_assets)
        current_weights = self.weights
        
        # 4. åˆä½µæˆå®Œæ•´ç‹€æ…‹
        state = np.concatenate([stock_features, cash_feature, current_weights]).astype(np.float32)
        
        return state
    
    def step(self, action):
        """åŸ·è¡Œä¸€æ­¥ - ä½¿ç”¨èåˆç‰ˆçå‹µå‡½æ•¸"""
        
        # æ·»åŠ NaNæª¢æŸ¥
        if np.any(np.isnan(action)) or np.any(np.isinf(action)):
            action = np.ones(self.action_dim) / self.action_dim
            
        # å°‡é€£çºŒå‹•ä½œè½‰æ›ç‚ºæœ‰æ•ˆçš„æŠ•è³‡çµ„åˆæ¬Šé‡
        try:
            action_clipped = np.clip(action, -10, 10)
            exp_action = np.exp(action_clipped - np.max(action_clipped))
            self.weights = exp_action / (np.sum(exp_action) + 1e-8)
            
            if np.any(np.isnan(self.weights)) or np.any(np.isinf(self.weights)) or np.sum(self.weights) < 0.99:
                self.weights = self.initial_weights.copy()
                
        except Exception as e:
            self.weights = self.initial_weights.copy()
        
        self.weights_history.append(self.weights.copy())
        
        # è¨ˆç®—æŠ•è³‡çµ„åˆæ”¶ç›Š
        stock_returns = self.returns[self.current_step]
        cash_return = self.daily_cash_return
        
        if np.any(np.isnan(stock_returns)) or np.any(np.isinf(stock_returns)):
            stock_returns = np.zeros_like(stock_returns)
        
        # è‚¡ç¥¨éƒ¨åˆ†æ”¶ç›Š
        stock_portfolio_return = np.dot(self.weights[:-1], stock_returns)
        
        # ç¾é‡‘éƒ¨åˆ†æ”¶ç›Š
        cash_portfolio_return = self.weights[-1] * cash_return
        
        # ç¸½æŠ•è³‡çµ„åˆæ”¶ç›Š
        portfolio_return = stock_portfolio_return + cash_portfolio_return
        portfolio_return = np.clip(portfolio_return, -0.5, 0.5)
        
        # è¨ˆç®—åŸºæº–æ”¶ç›Š
        equal_stock_weight = 0.8 / self.n_stocks
        benchmark_weights = np.array([equal_stock_weight] * self.n_stocks + [0.2])
        benchmark_stock_return = np.dot(benchmark_weights[:-1], stock_returns)
        benchmark_cash_return = benchmark_weights[-1] * cash_return
        benchmark_return = benchmark_stock_return + benchmark_cash_return
        benchmark_return = np.clip(benchmark_return, -0.5, 0.5)
        
        # æ›´æ–°æŠ•è³‡çµ„åˆåƒ¹å€¼
        self.portfolio_value *= (1 + portfolio_return)
        
        if np.isnan(self.portfolio_value) or np.isinf(self.portfolio_value) or self.portfolio_value <= 0:
            self.portfolio_value = self.portfolio_values_history[-1] if self.portfolio_values_history else 10000.0
            
        self.portfolio_values_history.append(self.portfolio_value)
        
        # ä½¿ç”¨èåˆç‰ˆçå‹µå‡½æ•¸
        try:
            reward, reward_components = self.reward_calculator.calculate_enhanced_reward(
                portfolio_value=self.portfolio_value,
                portfolio_return=portfolio_return,
                weights=self.weights,
                benchmark_return=benchmark_return,
                stock_returns=stock_returns,
                step=self.current_step
            )
            
            if np.isnan(reward) or np.isinf(reward):
                reward = 0.0
                reward_components = {k: 0.0 for k in ['sharpe', 'active_selection', 'mdd_penalty', 'cash_timing']}
                
        except Exception as e:
            reward = 0.0
            reward_components = {k: 0.0 for k in ['sharpe', 'active_selection', 'mdd_penalty', 'cash_timing']}
        
        # è¨˜éŒ„è©³ç´°çå‹µä¿¡æ¯
        self.reward_history.append({
            'step': self.current_step,
            'total_reward': reward,
            'components': reward_components.copy()
        })
        
        self.current_step += 1
        done = self.current_step >= self.n_days - 1
        
        # æ§‹é€ ä¸‹ä¸€å€‹ç‹€æ…‹
        next_state = self._get_current_state() if not done else np.zeros(self.state_dim, dtype=np.float32)
        
        # è©³ç´°çš„ç’°å¢ƒä¿¡æ¯è¼¸å‡º
        info = {
            'portfolio_value': self.portfolio_value,
            'weights': self.weights.copy(),
            'portfolio_return': portfolio_return,
            'benchmark_return': benchmark_return,
            'excess_return': portfolio_return - benchmark_return,
            'cash_ratio': self.weights[-1],
            'reward_components': reward_components,
            'total_reward': reward,
            'trading_day': self.current_step,  # ğŸ†• æ·»åŠ äº¤æ˜“æ—¥ä¿¡æ¯
            
            "market_conditions": {
                "market_return": float(np.mean(stock_returns)),
                "market_volatility": float(np.std(stock_returns)),
                "individual_stock_returns": stock_returns.tolist(),
                "cash_return": float(cash_return),
                "market_trend": "bullish" if np.mean(stock_returns) > 0.01 else "bearish" if np.mean(stock_returns) < -0.01 else "neutral"
            },
            
            "reward_breakdown": {
                "sharpe_component": float(reward_components.get('sharpe', 0)),
                "active_selection_component": float(reward_components.get('active_selection', 0)),
                "mdd_penalty_component": float(reward_components.get('mdd_penalty', 0)),
                "cash_timing_component": float(reward_components.get('cash_timing', 0)),
                "reward_approach": "active_selection_fusion"
            },
            
            "action_analysis": {
                "action_received": action.tolist() if isinstance(action, np.ndarray) else action,
                "final_weights": self.weights.tolist(),
                "weight_changes": (self.weights - self.weights_history[-2]).tolist() if len(self.weights_history) > 1 else [0.0] * len(self.weights),
                "portfolio_concentration": float(np.max(self.weights)),
                "weight_entropy": float(-np.sum(self.weights * np.log(self.weights + 1e-8))),
                "stock_concentration": float(np.sum(self.weights[:-1])),
                "individual_stock_weights": {
                    self.stock_names[i]: float(self.weights[i]) 
                    for i in range(len(self.stock_names))
                }
            },
            
            "performance_metrics": {
                "current_day": self.current_step,  # ğŸ†• ç•¶å‰ç¬¬å¹¾å¤©
                "total_days": self.n_days,        # ğŸ†• ç¸½å¤©æ•¸
                "portfolio_growth": float((self.portfolio_value / 10000.0 - 1) * 100),
                "is_outperforming": bool(portfolio_return > benchmark_return),
                "days_remaining": self.n_days - 1 - self.current_step,
                "max_drawdown_current": float(self.reward_calculator.mdd_portfolio_values[-1] - self.reward_calculator.mdd_running_peak) / self.reward_calculator.mdd_running_peak if self.reward_calculator.mdd_portfolio_values else 0.0
            }
        }
        
        return next_state, reward, done, info
    
    def calculate_equal_weight_benchmark(self):
        """è¨ˆç®—ç­‰æ¬Šé‡åŸºæº–æ”¶ç›Š"""
        equal_stock_weight = 0.8 / self.n_stocks
        equal_weights = np.array([equal_stock_weight] * self.n_stocks + [0.2])
        benchmark_values = [10000.0]
        
        for i in range(len(self.returns)):
            stock_returns = self.returns[i]
            cash_return = self.daily_cash_return
            
            stock_return = np.dot(equal_weights[:-1], stock_returns)
            cash_return_portion = equal_weights[-1] * cash_return
            total_return = stock_return + cash_return_portion
            
            new_value = benchmark_values[-1] * (1 + total_return)
            benchmark_values.append(new_value)
            
        return benchmark_values

def create_td3_learning_monitor_with_early_stopping(exp_dir, episode_data, training_losses, reward_history, test_weights_data, validation_scores, early_stopping_info):
    """ğŸ§  TD3å­¸ç¿’ç›£æ§åœ–è¡¨ - åŒ…å«Early Stoppingä¿¡æ¯"""
    plt.style.use('default')
    
    # æå–é—œéµæ•¸æ“š
    episodes = [data['episode'] for data in episode_data]
    returns = [data['return'] for data in episode_data]
    portfolio_values = [data['portfolio_value'] for data in episode_data]
    cash_ratios = [data['final_cash_ratio'] for data in episode_data]
    
    # å‰µå»º3x3å­åœ–
    fig, axes = plt.subplots(3, 3, figsize=(24, 18))
    fig.suptitle('ğŸ§  TD3 Learning Monitor with Early Stopping (Twin Delayed DDPG)', fontsize=16, fontweight='bold', y=0.98)
    
    # â‘  Episode Reward å­¸ç¿’è¶¨å‹¢ + Early Stopping æ¨™è¨˜
    ax1 = axes[0, 0]
    ax1.plot(episodes, returns, 'lightblue', alpha=0.6, linewidth=1, label='Raw Rewards')
    
    # ç§»å‹•å¹³å‡ç·š
    if len(returns) >= 10:
        window = max(5, len(returns) // 10)
        moving_avg = pd.Series(returns).rolling(window=window, center=True).mean()
        ax1.plot(episodes, moving_avg, 'red', linewidth=3, label=f'Moving Avg ({window})')
        
        # ç·šæ€§è¶¨å‹¢ç·š
        z = np.polyfit(episodes, returns, 1)
        trend_line = np.poly1d(z)
        ax1.plot(episodes, trend_line(episodes), 'green', linestyle='--', linewidth=2, 
                label=f'Trend (slope: {z[0]:.3f})')
        
        # å­¸ç¿’åˆ¤æ–·
        learning_status = "âœ… Learning!" if z[0] > 0 else "âš ï¸ Not Learning"
        ax1.text(0.02, 0.98, learning_status, transform=ax1.transAxes, 
                bbox=dict(boxstyle='round', facecolor='lightgreen' if z[0] > 0 else 'yellow'),
                fontsize=12, fontweight='bold', verticalalignment='top')
    
    # ğŸ†• æ¨™è¨˜Early Stoppingé»
    if early_stopping_info['early_stopped']:
        ax1.axvline(x=early_stopping_info['best_episode'], color='red', linestyle=':', 
                   linewidth=2, alpha=0.7, label=f'Best Episode ({early_stopping_info["best_episode"]})')
        ax1.axvline(x=len(episodes), color='orange', linestyle='--', 
                   linewidth=2, alpha=0.7, label='Early Stop')
    
    ax1.set_title('â‘  Episode Reward Learning Trend + Early Stopping', fontweight='bold')
    ax1.set_xlabel('Training Episode')
    ax1.set_ylabel('Reward')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # â‘¡ ğŸ†• é©—è­‰åˆ†æ•¸è¶‹åŠ¿
    ax2 = axes[0, 1]
    if validation_scores:
        val_episodes = [score['episode'] for score in validation_scores]
        val_scores = [score['score'] for score in validation_scores]
        
        ax2.plot(val_episodes, val_scores, 'purple', linewidth=2.5, marker='o', markersize=4, label='Validation Score')
        
        # æ¨™è¨˜æœ€ä½³åˆ†æ•¸
        best_idx = np.argmax(val_scores)
        ax2.plot(val_episodes[best_idx], val_scores[best_idx], 'red', marker='*', markersize=12, label='Best Score')
        
        # é¡¯ç¤ºEarly Stoppingä¿¡æ¯
        metric_name = 'Sharpe Ratio' if EARLY_STOPPING_CONFIG['use_sharpe_ratio'] else 'Avg Reward'
        ax2.text(0.02, 0.98, f'Best {metric_name}: {early_stopping_info["best_validation_score"]:.4f}\nBest Episode: {early_stopping_info["best_episode"]}\nEarly Stopped: {early_stopping_info["early_stopped"]}', 
                transform=ax2.transAxes,
                bbox=dict(boxstyle='round', facecolor='lightgreen' if early_stopping_info["early_stopped"] else 'lightblue'),
                fontsize=10, verticalalignment='top')
        
        ax2.set_title('â‘¡ Validation Score Trend', fontweight='bold')
        ax2.set_xlabel('Training Episode')
        ax2.set_ylabel(metric_name)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
    
    # â‘¢ Cash Allocation æ™ºèƒ½ç¨‹åº¦ï¼ˆæ¸¬è©¦æœŸé–“çš„å¤©æ•¸ï¼‰
    ax3 = axes[1, 0]
    if len(test_weights_data) > 0:
        test_days = list(range(1, len(test_weights_data) + 1))
        cash_pct_test = [w[-1] * 100 for w in test_weights_data]
        ax3.plot(test_days, cash_pct_test, 'orange', linewidth=2, label='Cash % (Testing)')
        ax3.axhline(y=20, color='gray', linestyle=':', alpha=0.7, label='Benchmark (20%)')
        
        # è¨ˆç®—ç¾é‡‘é…ç½®çš„æ™ºèƒ½åº¦
        if len(cash_pct_test) > 10:
            cash_std = np.std(cash_pct_test)
            cash_range = max(cash_pct_test) - min(cash_pct_test)
            adaptability = "ğŸ¤– Adaptive" if cash_std > 5 else "ğŸ˜´ Static"
            
            ax3.text(0.02, 0.98, f'{adaptability}\nStd: {cash_std:.1f}%\nRange: {cash_range:.1f}%', 
                    transform=ax3.transAxes,
                    bbox=dict(boxstyle='round', facecolor='lightblue'),
                    fontsize=10, verticalalignment='top')
        
        ax3.set_title('â‘¢ Cash Allocation Intelligence (Testing)', fontweight='bold')
        ax3.set_xlabel('Trading Day')
        ax3.set_ylabel('Cash Allocation (%)')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
    
    # â‘£ è¨“ç·´æ”¶æ–‚ç‹€æ³ (TD3 specific)
    ax4 = axes[1, 1]
    if training_losses:
        update_episodes = list(range(1, len(training_losses) + 1))
        actor_losses = [loss['actor_loss'] for loss in training_losses]
        critic1_losses = [loss.get('critic1_loss', 0) for loss in training_losses]
        critic2_losses = [loss.get('critic2_loss', 0) for loss in training_losses]
        
        ax4.plot(update_episodes, actor_losses, 'red', linewidth=2, label='Actor Loss', marker='o', markersize=2)
        ax4.plot(update_episodes, critic1_losses, 'blue', linewidth=2, label='Critic1 Loss', marker='s', markersize=2)
        ax4.plot(update_episodes, critic2_losses, 'purple', linewidth=2, label='Critic2 Loss', marker='^', markersize=2)
        
        # æ”¶æ–‚åˆ¤æ–·
        if len(actor_losses) >= 5:
            recent_actor_std = np.std(actor_losses[-5:])
            recent_c1_std = np.std(critic1_losses[-5:])
            convergence_status = "âœ… Converging" if (recent_actor_std < 0.1 and recent_c1_std < 0.1) else "âš ï¸ Unstable"
            ax4.text(0.02, 0.98, f'{convergence_status}\nActor Std: {recent_actor_std:.3f}\nCritic1 Std: {recent_c1_std:.3f}',
                    transform=ax4.transAxes,
                    bbox=dict(boxstyle='round', facecolor='lightgreen' if recent_actor_std < 0.1 and recent_c1_std < 0.1 else 'yellow'),
                    fontsize=10, verticalalignment='top')
    
    ax4.set_title('â‘£ TD3 Training Convergence', fontweight='bold')
    ax4.set_xlabel('Training Update Step')
    ax4.set_ylabel('Loss')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # â‘¤ Qå€¼å¥åº·åº¦ (TD3 specific)
    ax5 = axes[2, 0]
    if training_losses:
        q1_values = [loss.get('avg_q1_value', 0) for loss in training_losses]
        q2_values = [loss.get('avg_q2_value', 0) for loss in training_losses]
        
        ax5.plot(update_episodes, q1_values, 'blue', linewidth=2, marker='o', markersize=2, label='Avg Q1-Value')
        ax5.plot(update_episodes, q2_values, 'purple', linewidth=2, marker='s', markersize=2, label='Avg Q2-Value')
        
        # Qå€¼å¥åº·åº¦è©•ä¼°
        avg_q1 = np.mean(q1_values) if q1_values else 0
        avg_q2 = np.mean(q2_values) if q2_values else 0
        q1_std = np.std(q1_values) if q1_values else 0
        q2_std = np.std(q2_values) if q2_values else 0
        
        # æ•´é«”å¥åº·åº¦åˆ¤æ–·
        q1_health = abs(avg_q1) < 10 and q1_std < 5
        q2_health = abs(avg_q2) < 10 and q2_std < 5
        overall_health = "ğŸ’š Healthy" if (q1_health and q2_health) else "ğŸ’› Caution" if (q1_health or q2_health) else "â¤ï¸ Dangerous"
        
        ax5.text(0.02, 0.98, f'{overall_health}\nAvg Q1: {avg_q1:.2f} (Â±{q1_std:.2f})\nAvg Q2: {avg_q2:.2f} (Â±{q2_std:.2f})',
                transform=ax5.transAxes,
                bbox=dict(boxstyle='round', facecolor='lightgreen' if (q1_health and q2_health) else 'yellow' if (q1_health or q2_health) else 'pink'),
                fontsize=10, verticalalignment='top')
    
    ax5.set_title('â‘¤ Twin Q-Value Health (TD3)', fontweight='bold')
    ax5.set_xlabel('Training Update Step')
    ax5.set_ylabel('Average Q-Value')
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    
    # â‘¥ æ¬Šé‡ç©©å®šæ€§åˆ†æï¼ˆæ¸¬è©¦æœŸé–“ï¼ŒæŒ‰å¤©æ•¸ï¼‰
    ax6 = axes[2, 1]
    if len(test_weights_data) > 10:
        test_days = list(range(1, len(test_weights_data) + 1))
        
        # è¨ˆç®—æ¯æ—¥æ¬Šé‡è®ŠåŒ–
        weight_changes = []
        for i in range(1, len(test_weights_data)):
            change = np.sum(np.abs(np.array(test_weights_data[i]) - np.array(test_weights_data[i-1])))
            weight_changes.append(change)
        
        change_days = list(range(2, len(test_weights_data) + 1))
        ax6.plot(change_days, weight_changes, 'purple', linewidth=2, alpha=0.8)
        
        # ç©©å®šæ€§è©•ä¼°
        avg_change = np.mean(weight_changes)
        stability_status = "ğŸ“ˆ Stable" if avg_change < 0.1 else "âš¡ Dynamic"
        ax6.text(0.02, 0.98, f'{stability_status}\nAvg Change: {avg_change:.3f}',
                transform=ax6.transAxes,
                bbox=dict(boxstyle='round', facecolor='lightgreen' if avg_change < 0.1 else 'yellow'),
                fontsize=10, verticalalignment='top')
        
        ax6.set_title('â‘¥ Weight Stability (Testing Days)', fontweight='bold')
        ax6.set_xlabel('Trading Day')
        ax6.set_ylabel('Daily Weight Change')
        ax6.grid(True, alpha=0.3)
    
    # â‘¦ èåˆç‰ˆçå‹µæˆåˆ†åˆ†æï¼ˆæ¸¬è©¦æœŸé–“ï¼‰
    ax7 = axes[0, 2]
    if reward_history and len(reward_history) > 0:
        try:
            # ==================================
            #  BUG FIX: å°é¾å¤§çš„ reward_history æ•¸æ“šé€²è¡Œé™æ¡æ¨£/å¹³æ»‘åŒ–
            # ==================================
            reward_df = pd.DataFrame([r['components'] for r in reward_history])
            
            # è¨ˆç®—ç§»å‹•å¹³å‡ï¼Œçª—å£å¤§å°è¨­ç‚ºç¸½æ­¥æ•¸çš„ 1% æˆ–è‡³å°‘ç‚º 100
            window_size = max(100, len(reward_df) // 100)
            
            # ä½¿ç”¨ rolling().mean() é€²è¡Œå¹³æ»‘åŒ–
            sharpe_rewards_smooth = reward_df['sharpe'].rolling(window=window_size, min_periods=1).mean()
            active_selection_rewards_smooth = reward_df['active_selection'].rolling(window=window_size, min_periods=1).mean()
            mdd_penalties_smooth = reward_df['mdd_penalty'].rolling(window=window_size, min_periods=1).mean()
            cash_timing_rewards_smooth = reward_df['cash_timing'].rolling(window=window_size, min_periods=1).mean()
            
            # ä½¿ç”¨åŸå§‹çš„ global steps ä½œç‚º x è»¸
            reward_steps = range(len(reward_df))

            ax7.plot(reward_steps, sharpe_rewards_smooth, 'g-', linewidth=2, alpha=0.8, label=f'Sharpe (MA {window_size})')
            ax7.plot(reward_steps, active_selection_rewards_smooth, 'b-', linewidth=2, alpha=0.8, label=f'Active Selection (MA {window_size})')
            ax7.plot(reward_steps, mdd_penalties_smooth, 'r-', linewidth=2, alpha=0.8, label=f'MDD Penalty (MA {window_size})')
            ax7.plot(reward_steps, cash_timing_rewards_smooth, 'orange', linewidth=2, alpha=0.8, label=f'Cash Timing (MA {window_size})')
            
            # å°‡æ¨™é¡Œä¿®æ­£ç‚ºæ›´èƒ½åæ˜ æ•¸æ“šä¾†æº
            ax7.set_title('â‘¦ Smoothed Reward Components (Training)', fontweight='bold')
            ax7.set_xlabel('Global Training Step') # x è»¸ç¾åœ¨æ˜¯å…¨å±€è¨“ç·´æ­¥æ•¸
            ax7.set_ylabel('Smoothed Reward Component')
            ax7.legend()
            ax7.grid(True, alpha=0.3)
                
        except Exception as e:
            ax7.text(0.5, 0.5, f'Reward analysis error:\n{str(e)[:50]}...', 
                    transform=ax7.transAxes, ha='center', va='center',
                    bbox=dict(boxstyle='round', facecolor='pink'))
            ax7.set_title('â‘¦ Fusion Reward Components (Error)', fontweight='bold')
    
    # â‘§ è‚¡ç¥¨ vs ç¾é‡‘æ¯”ä¾‹å‹•æ…‹ï¼ˆæ¸¬è©¦æœŸé–“ï¼‰
    ax8 = axes[1, 2]
    if len(test_weights_data) > 0:
        test_days = list(range(1, len(test_weights_data) + 1))
        stock_weights = [sum(w[:-1]) for w in test_weights_data]
        cash_weights = [w[-1] for w in test_weights_data]
        
        ax8.plot(test_days, stock_weights, label='Total Stocks', color='#C41E3A', linewidth=2.5, alpha=0.9)
        ax8.plot(test_days, cash_weights, label='Cash', color='#B8860B', linewidth=2.5, linestyle='--', alpha=0.9)
        
        ax8.axhline(y=0.8, color='#191970', linestyle=':', alpha=0.8, linewidth=2, label='Benchmark Stocks (80%)')
        ax8.axhline(y=0.2, color='#FF8C00', linestyle=':', alpha=0.8, linewidth=2, label='Benchmark Cash (20%)')
        
        ax8.set_title('â‘§ Stocks vs Cash Dynamics (Testing)', fontweight='bold')
        ax8.set_xlabel('Trading Day')
        ax8.set_ylabel('Weight')
        ax8.legend()
        ax8.grid(True, alpha=0.3)
    
    # â‘¨ ğŸ†• TD3 + Early Stoppingæ¶æ§‹èªªæ˜
    ax9 = axes[2, 2]
    ax9.text(0.1, 0.95, 'ğŸ¤– TD3 + Early Stopping Architecture', fontsize=14, fontweight='bold', transform=ax9.transAxes, color='red')
    ax9.text(0.1, 0.85, 'âœ… Actor Network: Ï€(a|s) - Deterministic', fontsize=11, transform=ax9.transAxes)
    ax9.text(0.1, 0.78, 'âœ… Target Actor: Ï€Ì„(a|s)', fontsize=11, transform=ax9.transAxes, color='blue')
    ax9.text(0.1, 0.71, 'âœ… Twin Critics: Qâ‚(s,a) & Qâ‚‚(s,a)', fontsize=11, transform=ax9.transAxes, color='purple')
    ax9.text(0.1, 0.64, 'âœ… Target Critics: QÌ„â‚(s,a) & QÌ„â‚‚(s,a)', fontsize=11, transform=ax9.transAxes, color='purple')
    ax9.text(0.1, 0.57, 'âŒ No V-Critic Networks', fontsize=11, transform=ax9.transAxes, color='red')
    ax9.text(0.1, 0.50, 'âŒ No Entropy Term', fontsize=11, transform=ax9.transAxes, color='red')
    ax9.text(0.1, 0.43, 'âœ… Policy Delay: Update every 2 steps', fontsize=11, transform=ax9.transAxes)
    ax9.text(0.1, 0.36, 'âœ… Target Smoothing: Noise clipping', fontsize=11, transform=ax9.transAxes)
    ax9.text(0.1, 0.29, 'ğŸ”„ Fusion Reward: 4 components', fontsize=11, transform=ax9.transAxes, color='green')
    ax9.text(0.1, 0.22, 'ğŸ“Š Rich State: 5 features/stock', fontsize=11, transform=ax9.transAxes, color='green')
    ax9.text(0.1, 0.15, 'â­ Deterministic policy with exploration', fontsize=11, transform=ax9.transAxes, color='blue')
    ax9.text(0.1, 0.08, f'ğŸ”„ Early Stopping: {early_stopping_info["early_stopping_config"]["patience"]} patience', fontsize=11, transform=ax9.transAxes, color='orange')
    ax9.text(0.1, 0.01, f'ğŸ’¾ Best Model: Episode {early_stopping_info["best_episode"]}', fontsize=11, transform=ax9.transAxes, color='green')
    ax9.set_title('â‘¨ TD3 Twin Delayed DDPG + Early Stopping', fontweight='bold')
    ax9.set_xlim(0, 1)
    ax9.set_ylim(0, 1)
    ax9.axis('off')
    
    plt.tight_layout()
    plt.savefig(f'{exp_dir}/plots/enhanced_td3_learning_monitor_early_stopping.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… Enhanced TD3 Learning Monitor with Early Stopping created!")

def run_td3_fusion_experiment_with_early_stopping(csv_file_path=None):
    """é‹è¡ŒTD3èåˆå¯¦é©— - Twin Delayed DDPGï¼Œé›†æˆ Early Stopping æ©Ÿåˆ¶"""
    
    print("ğŸ¯ TD3 Portfolio Management with Early Stopping - Twin Delayed Deep Deterministic Policy Gradient ğŸ¯")
    print("=" * 80)
    print("ğŸ“ˆ Rich State Features (5 per stock) + Fusion Reward Function")
    print("ğŸ¯ State: Momentum(5d,20d) + Volatility + RSI + Bollinger%B + Current Weights")
    print("ğŸ¯ Reward: Sharpe + ActiveSelection + MaxDrawdown + CashTiming")
    print("ğŸ”„ Algorithm: TD3 (Twin Delayed DDPG)")
    print("ğŸ—ï¸ Networks: Actor + Target Actor + Twin Critics + Target Critics")
    print("ğŸ“Š Charts: X-axis using Trading Days (1, 2, 3, ..., N)")
    print("âœ… Deterministic Policy with Exploration Noise")
    print("âœ… Policy Delay + Target Smoothing + Twin Critics")
    print("âŒ No V-Networks (unlike SAC)")
    print("âŒ No Entropy Term (deterministic policy)")
    print("ğŸ”„ ğŸ†• Early Stopping: é©—è­‰é›†è©•ä¼°è‡ªå‹•åœæ­¢è¨“ç·´")
    print("ğŸ“Š ğŸ†• Best Model Selection: ä¿å­˜ä¸¦è¼‰å…¥æœ€ä½³æ¨¡å‹")
    print("=" * 80)
    
    setup_gpu()
    
    # è¨­ç½®å¯¦é©—ç’°å¢ƒ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    exp_dir = f"td3_fusion_early_stop_{timestamp}"
    
    dirs = [exp_dir, f"{exp_dir}/data", f"{exp_dir}/models", f"{exp_dir}/plots"]
    for dir_path in dirs:
        os.makedirs(dir_path, exist_ok=True)
    
    # èåˆç‰ˆé…ç½®
    config = EnhancedFinancialConfig()
    
    # é»˜èªè·¯å¾‘
    if csv_file_path is None:
        csv_file_path = "eight_stock_prices.csv"
    
    if not os.path.exists(csv_file_path):
        print(f"âœ— File not found: {csv_file_path}")
        return
    
    print("\n=== Creating Enhanced Environments with Early Stopping ===")
    start_time = time.time()
    
    try:
        train_env = EnhancedPortfolioEnvironment(csv_file_path, mode='train', config=config)
        val_env = EnhancedPortfolioEnvironment(csv_file_path, mode='val', config=config)
        test_env = EnhancedPortfolioEnvironment(csv_file_path, mode='test', config=config)
        
        print(f"âœ“ Enhanced environments created in {time.time() - start_time:.1f}s")
        print(f"âœ“ Detected stocks: {train_env.stock_names}")
        print(f"âœ“ Enhanced state dimension: {train_env.state_dim}")
        print(f"âœ“ Testing days: {test_env.n_days}")
        
    except Exception as e:
        print(f"âœ— Environment creation failed: {e}")
        return
    
    print(f"\n=== Creating TD3 Agent with Early Stopping Support ===")
    print(f"ğŸ”§ Setting up TD3 (Twin Delayed DDPG) agent...")
    
    # å‰µå»ºTD3 Agent
    agent = TD3Agent(
        state_dim=train_env.state_dim,
        action_dim=train_env.action_dim,
        learning_rate=1e-4
    )
    
    # ğŸ”§ é©—è­‰TD3æ¶æ§‹
    print(f"âœ… Actor network: {hasattr(agent, 'actor')}")
    print(f"âœ… Target Actor network: {hasattr(agent, 'actor_target')}")
    print(f"âœ… Critic1 network: {hasattr(agent, 'critic1')}")
    print(f"âœ… Critic2 network: {hasattr(agent, 'critic2')}")
    print(f"âœ… Target Critic1 network: {hasattr(agent, 'critic1_target')}")
    print(f"âœ… Target Critic2 network: {hasattr(agent, 'critic2_target')}")
    print(f"âŒ No V-Critic (correct): {not hasattr(agent, 'v_critic')}")
    print(f"âŒ No Alpha (correct): {not hasattr(agent, 'log_alpha')}")
    print(f"ğŸ”„ Early Stopping methods: {hasattr(agent, 'save_best_models')} & {hasattr(agent, 'load_best_models')}")
    
    print(f"\n=== TD3 Training with Early Stopping ===")
    print(f"State features: {train_env.n_stocks} stocks Ã— 5 features + {train_env.n_assets} weights + 1 cash = {train_env.state_dim}")
    print(f"Network architecture: Actor + Target Actor + Twin Critics + Target Critics")
    print(f"Reward focus: Sharpe({config.sharpe_weight:.0%}) + ActiveSelection({config.return_weight:.0%}) + MDD({config.mdd_weight:.0%}) + Cash({config.cash_timing_weight:.0%})")
    print(f"Algorithm: TD3 (Deterministic policy with exploration noise)")
    print(f"Key features: Policy Delay + Target Smoothing + Twin Critics")
    
    # ğŸ†• Early Stopping è¨­ç½®
    print(f"\nğŸ”„ Early Stopping Configuration:")
    print(f"  ğŸ“Š Evaluation Interval: {EARLY_STOPPING_CONFIG['eval_interval']} episodes")
    print(f"  â³ Patience: {EARLY_STOPPING_CONFIG['patience']} evaluations")
    print(f"  ğŸ“ Minimum Episodes: {EARLY_STOPPING_CONFIG['min_episodes']}")
    print(f"  ğŸ“ˆ Metric: {'Sharpe Ratio' if EARLY_STOPPING_CONFIG['use_sharpe_ratio'] else 'Average Reward'}")
    print(f"  ğŸ’¾ Save Best Models: {EARLY_STOPPING_CONFIG['save_best_models']}")
    
    # è¨“ç·´åƒæ•¸
    max_episodes = 200  # æœ€å¤§ episode æ•¸ï¼Œä½†å¯èƒ½æå‰åœæ­¢
    max_steps_per_episode = len(train_env.features)
    start_training_step = 1000
    training_frequency = 10
    
    # ğŸ†• Early Stopping è®Šæ•¸
    best_val_score = -np.inf
    best_episode = -1
    patience_counter = 0
    eval_interval = EARLY_STOPPING_CONFIG['eval_interval']
    patience = EARLY_STOPPING_CONFIG['patience']
    min_episodes = EARLY_STOPPING_CONFIG['min_episodes']
    use_sharpe_ratio = EARLY_STOPPING_CONFIG['use_sharpe_ratio']
    save_best_models = EARLY_STOPPING_CONFIG['save_best_models']
    
    # æ•¸æ“šæ”¶é›†
    episode_data = []
    weights_data = []
    training_losses = []
    episode_reward_history = []
    feedback_data = []
    validation_scores = []  # ğŸ†• è¨˜éŒ„é©—è­‰åˆ†æ•¸
    
    global_step = 0
    
    print(f"\n=== Enhanced TD3 Training Loop with Early Stopping ===")
    training_start = time.time()
    
    try:
        for episode in range(max_episodes):
            state = train_env.reset()
            episode_return = 0
            episode_steps = 0
            
            for step in range(max_steps_per_episode):
                # ç²å–actionï¼ˆå¸¶æ¢ç´¢å™ªè²ï¼‰
                action, action_info = agent.get_action(state, add_noise=True)
                
                # ç’°å¢ƒstep
                next_state, reward, done, info = train_env.step(action)
                
                # æ”¶é›†åé¥‹æ•¸æ“š
                feedback_entry = {
                    "episode": episode,
                    "step": step,
                    "global_step": global_step,
                    "action_info": action_info,
                    "env_info": info,
                    "reward": float(reward),
                    "enhanced_features": True,
                    "reward_approach": "active_selection_fusion",
                    "algorithm": "TD3",
                    "early_stopping": True,  # ğŸ†• æ¨™è¨˜ä½¿ç”¨äº† Early Stopping
                }
                feedback_data.append(feedback_entry)
                
                # æ·»åŠ ç¶“é©—åˆ°å›æ”¾ç·©è¡å€
                agent.add_experience(state, action, reward, next_state, done)
                
                # TD3è¨“ç·´
                if global_step >= start_training_step and global_step % training_frequency == 0:
                    train_info = agent.train()
                    if train_info:
                        training_losses.append(train_info)
                
                state = next_state
                episode_return += reward
                episode_steps += 1
                global_step += 1
                
                if done:
                    break
            
            # è¨˜éŒ„episodeæ•¸æ“š
            episode_info = {
                'episode': episode,
                'return': episode_return,
                'portfolio_value': train_env.portfolio_value,
                'final_weights': train_env.weights.copy(),
                'final_cash_ratio': train_env.weights[-1],
                'steps': episode_steps,
                'global_step': global_step
            }
            episode_data.append(episode_info)
            episode_reward_history.extend(train_env.reward_history)
            
            # ğŸ†• Early Stopping è©•ä¼°
            if (episode + 1) % eval_interval == 0 and episode >= min_episodes:
                print(f"\nğŸ” Validation Evaluation at Episode {episode + 1}")
                
                # åœ¨é©—è­‰é›†ä¸Šè©•ä¼°
                val_score = evaluate_on_validation(agent, val_env)
                validation_scores.append({
                    'episode': episode + 1,
                    'score': val_score,
                    'metric': 'sharpe_ratio' if use_sharpe_ratio else 'avg_reward'
                })
                
                print(f"  ğŸ“Š Validation {'Sharpe Ratio' if use_sharpe_ratio else 'Average Reward'}: {val_score:.4f}")
                print(f"  ğŸ† Best Score So Far: {best_val_score:.4f} (Episode {best_episode})")
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºæœ€ä½³åˆ†æ•¸
                if val_score > best_val_score:
                    best_val_score = val_score
                    best_episode = episode + 1
                    patience_counter = 0
                    
                    # ğŸ†• ä¿å­˜æœ€ä½³æ¨¡å‹
                    if save_best_models:
                        agent.save_best_models(f"{exp_dir}/models")
                    
                    print(f"  âœ… NEW BEST! Score: {val_score:.4f} at Episode {episode + 1}")
                    print(f"  ğŸ’¾ Best models saved to {exp_dir}/models/")
                else:
                    patience_counter += 1
                    print(f"  â³ No improvement. Patience: {patience_counter}/{patience}")
                    
                    # ğŸ”„ Early Stopping æ¢ä»¶æª¢æŸ¥
                    if patience_counter >= patience:
                        print(f"\nğŸ›‘ EARLY STOPPING TRIGGERED!")
                        print(f"  ğŸ“Š Stopped at Episode: {episode + 1}")
                        print(f"  ğŸ† Best Score: {best_val_score:.4f}")
                        print(f"  ğŸ¯ Best Episode: {best_episode}")
                        print(f"  â³ Patience Exceeded: {patience_counter}/{patience}")
                        break
            
            # é€²åº¦å ±å‘Š
            if episode % 10 == 0:
                elapsed = time.time() - training_start
                current_weights = train_env.weights
                buffer_size = len(agent.replay_buffer)
                
                print(f"\nEpisode {episode:3d}: Portfolio ${train_env.portfolio_value:8.0f}, "
                      f"Return {episode_return:6.2f}, Time {elapsed:.0f}s")
                print(f"  TD3 State Dim: {train_env.state_dim}, Buffer: {buffer_size}, Training Step: {agent.training_step}")
                
                # é¡¯ç¤ºè‚¡ç¥¨æ¬Šé‡åˆ†å¸ƒ
                print(f"  ğŸ“Š Stock Weights:")
                for i, (stock, weight) in enumerate(zip(train_env.stock_names[:4], current_weights[:4])):
                    print(f"    {stock}: {weight:.1%}", end="  ")
                if len(train_env.stock_names) > 4:
                    print(f"\n    ... and {len(train_env.stock_names) - 4} more stocks")
                print(f"  ğŸ’° Cash: {current_weights[-1]:.1%}")
                
                # ğŸ†• é¡¯ç¤º Early Stopping ç‹€æ…‹
                if episode >= min_episodes:
                    print(f"  ğŸ”„ Early Stopping Status:")
                    print(f"    ğŸ“Š Best Score: {best_val_score:.4f} (Episode {best_episode})")
                    print(f"    â³ Patience: {patience_counter}/{patience}")
                    next_eval = ((episode // eval_interval) + 1) * eval_interval
                    print(f"    ğŸ“… Next Evaluation: Episode {next_eval}")
                
                # å±•ç¤ºèåˆç‰ˆçå‹µæˆåˆ†
                if len(episode_reward_history) > 0:
                    recent_rewards = episode_reward_history[-20:]
                    if recent_rewards and 'components' in recent_rewards[0]:
                        avg_components = {}
                        for component in recent_rewards[0]['components'].keys():
                            component_values = [r['components'][component] for r in recent_rewards 
                                              if not np.isnan(r['components'][component])]
                            if component_values:
                                avg_components[component] = np.mean(component_values)
                        
                        print("ğŸ¯ TD3 Fusion Reward Components:")
                        for component, value in avg_components.items():
                            emoji = "ğŸ“ˆ" if component == "sharpe" else "ğŸ¯" if component == "active_selection" else "â¬‡ï¸" if component == "mdd_penalty" else "â°"
                            if not np.isnan(value):
                                print(f"  {emoji} {component}: {value:.4f}")
                        print(f"  ğŸ—ï¸ Architecture: TD3 (Twin Delayed DDPG)")
                        print(f"  ğŸ”§ Deterministic Policy + Exploration Noise")
    
    except Exception as e:
        print(f"Training error: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # ğŸ†• è¼‰å…¥æœ€ä½³æ¨¡å‹é€²è¡Œæ¸¬è©¦
    print(f"\n=== Loading Best Model for Testing ===")
    if save_best_models and best_episode > 0:
        print(f"ğŸ”„ Loading best model from Episode {best_episode} (Score: {best_val_score:.4f})")
        agent.load_best_models(f"{exp_dir}/models")
    else:
        print(f"âš ï¸ Using current model for testing (no best model saved)")
    
    # æ¸¬è©¦éšæ®µ
    print(f"\n=== Testing Enhanced TD3 Model with Early Stopping ===")
    test_state = test_env.reset()
    test_done = False
    test_steps = 0
    test_feedback_data = []
    
    while not test_done:
        test_action, test_action_info = agent.get_action(test_state, add_noise=False)  # æ¸¬è©¦æ™‚ä¸åŠ å™ªè²
        test_next_state, test_reward, test_done, test_info = test_env.step(test_action)
        
        # æ”¶é›†æ¸¬è©¦åé¥‹æ•¸æ“š
        test_feedback_entry = {
            "step": test_steps,
            "trading_day": test_steps + 1,  # ğŸ†• æ·»åŠ äº¤æ˜“æ—¥ä¿¡æ¯
            "action_info": test_action_info,
            "env_info": test_info,
            "reward": float(test_reward),
            "phase": "testing",
            "enhanced_features": True,
            "reward_approach": "active_selection_fusion",
            "algorithm": "TD3",
            "early_stopping_used": True,  # ğŸ†• æ¨™è¨˜ä½¿ç”¨äº† Early Stopping
            "best_episode_used": best_episode,
            "best_score": best_val_score,
        }
        test_feedback_data.append(test_feedback_entry)
        
        test_state = test_next_state
        test_steps += 1
    
    # ä¿å­˜æ¸¬è©¦æœŸé–“çš„æ¬Šé‡æ•¸æ“šï¼ˆç”¨æ–¼åœ–è¡¨ï¼‰
    test_weights_data = test_env.weights_history
    
    # ä¿å­˜å¢å¼·ç‰ˆçµæœ
    print(f"\n=== Saving Enhanced TD3 Results with Early Stopping ===")
    
    # 1. ä¿å­˜episodeæ•¸æ“š
    episode_df = pd.DataFrame(episode_data)
    episode_df.to_csv(f'{exp_dir}/data/enhanced_td3_episode_data_early_stop.csv', index=False)
    
    # ğŸ†• 2. ä¿å­˜é©—è­‰åˆ†æ•¸æ­·å²
    if validation_scores:
        validation_df = pd.DataFrame(validation_scores)
        validation_df.to_csv(f'{exp_dir}/data/validation_scores_history.csv', index=False)
    
    # ğŸ†• 3. ä¿å­˜ Early Stopping æ‘˜è¦
    early_stopping_summary = {
        'early_stopping_config': EARLY_STOPPING_CONFIG,
        'best_validation_score': float(best_val_score),
        'best_episode': int(best_episode),
        'total_episodes_trained': len(episode_data),
        'patience_counter_final': int(patience_counter),
        'early_stopped': bool(patience_counter >= patience),
        'metric_used': 'sharpe_ratio' if use_sharpe_ratio else 'avg_reward',
        'models_saved': save_best_models,
        'training_time_saved': f"Potentially saved {max_episodes - len(episode_data)} episodes"
    }
    
    with open(f'{exp_dir}/data/early_stopping_summary.json', 'w') as f:
        json.dump(early_stopping_summary, f, indent=2, ensure_ascii=False)
    
    # 4. ä¿å­˜æ¸¬è©¦æ¬Šé‡æ•¸æ“š
    assets = train_env.stock_names + ['CASH']
    test_weights_df = pd.DataFrame(test_weights_data, columns=assets)
    test_weights_df['trading_day'] = range(1, len(test_weights_data) + 1)
    test_weights_df.to_csv(f'{exp_dir}/data/test_weights_by_days_early_stop.csv', index=False)
    
    if training_losses:
        losses_df = pd.DataFrame(training_losses)
        losses_df.to_csv(f'{exp_dir}/data/enhanced_td3_training_losses_early_stop.csv', index=False)
    
    # 5. ä¿å­˜çå‹µæ­·å²
    reward_df = pd.DataFrame([
        {
            'step': r['step'],
            'total_reward': r['total_reward'],
            **r['components']
        } for r in episode_reward_history
    ])
    reward_df.to_csv(f'{exp_dir}/data/enhanced_td3_detailed_rewards_early_stop.csv', index=False)
    
    # 6. ä¿å­˜åé¥‹æ•¸æ“š
    if feedback_data:
        feedback_df = pd.DataFrame(feedback_data)
        feedback_df.to_csv(f'{exp_dir}/data/enhanced_td3_feedback_loop_data_early_stop.csv', index=False)
    
    if test_feedback_data:
        test_feedback_df = pd.DataFrame(test_feedback_data)
        test_feedback_df.to_csv(f'{exp_dir}/data/enhanced_td3_test_feedback_by_days_early_stop.csv', index=False)
    
    # ğŸ†• 7. ä¿å­˜TD3æ¨¡å‹ (æœ€çµ‚æ¨¡å‹ï¼Œé™¤äº†æœ€ä½³æ¨¡å‹)
    agent.actor.save(f'{exp_dir}/models/td3_actor_final.keras')
    agent.actor_target.save(f'{exp_dir}/models/td3_actor_target_final.keras')
    agent.critic1.save(f'{exp_dir}/models/td3_critic1_final.keras')
    agent.critic2.save(f'{exp_dir}/models/td3_critic2_final.keras')
    agent.critic1_target.save(f'{exp_dir}/models/td3_critic1_target_final.keras')
    agent.critic2_target.save(f'{exp_dir}/models/td3_critic2_target_final.keras')
    
    # ğŸ†• 8. å‰µå»ºTD3å­¸ç¿’ç›£æ§åœ–è¡¨ (åŒ…å«Early Stoppingä¿¡æ¯)
    print("ğŸ§  Creating enhanced TD3 learning monitor with Early Stopping info...")
    create_td3_learning_monitor_with_early_stopping(exp_dir, episode_data, training_losses, episode_reward_history, test_weights_data, validation_scores, early_stopping_summary)
    
    # ğŸ†• 9. å‰µå»ºä»¥å¤©æ•¸ç‚ºxè»¸çš„å¯è¦–åŒ–åœ–è¡¨
    print("ğŸ“Š Creating enhanced TD3 visualizations with trading days as x-axis...")
    create_enhanced_visualizations_with_days(exp_dir, episode_data, test_weights_data, training_losses, train_env.stock_names, test_steps)
    
    # ğŸ†• 10. å‰µå»ºä»¥å¤©æ•¸ç‚ºxè»¸çš„è©³ç´°å›æ¸¬å°æ¯”
    print("ğŸ“Š Creating detailed TD3 backtest comparison by trading days...")
    backtest_results = create_detailed_backtest_comparison_by_days(exp_dir, test_env, train_env.stock_names)
    
    # ğŸ†• 11. å‰µå»ºå°ˆæ¥­é¢¨æ ¼å †ç–Šåœ–
    # æˆ‘å€‘éœ€è¦å¾æ¸¬è©¦ç’°å¢ƒä¸­ç²å–çœŸå¯¦çš„æ—¥æœŸç´¢å¼•
    test_dates = test_env.price_subset.index
    create_professional_stacked_chart_final(exp_dir, test_weights_data, train_env.stock_names, test_dates)
    
    # 12. ç”Ÿæˆç³»çµ±è¼¸å‡º
    final_value = test_env.portfolio_value
    total_return = (final_value / 10000.0) - 1
    final_cash_ratio = test_env.weights[-1]
    
    portfolio_values = test_env.portfolio_values_history
    peak_values = np.maximum.accumulate(portfolio_values)
    drawdowns = (np.array(portfolio_values) - peak_values) / peak_values
    max_drawdown = np.min(drawdowns)
    
    # å®Œæ•´ç³»çµ±è¼¸å‡º
    system_output = {
        "ğŸ¯ quant_model_layer": config.output_objectives(),
        "ğŸ¤– rl_agent_layer": {
            "algorithm": "TD3 (Twin Delayed DDPG)",
            "state_dim": agent.state_dim,
            "action_dim": agent.action_dim,
            "enhanced_features": True,
            "features_per_stock": 5,
            "fusion_approach": True,
            "algorithm_type": "deterministic_policy_gradient",
            "exploration_method": "gaussian_noise",
            "early_stopping_enabled": True,  # ğŸ†•
            "state_composition": {
                "stock_features": train_env.n_stocks * 5,
                "cash_feature": 1,
                "weight_features": train_env.n_assets,
                "total": train_env.state_dim
            },
            "network_architecture": {
                "actor": "Ï€(a|s) - Deterministic policy network",
                "actor_target": "Ï€Ì„(a|s) - Target policy network",
                "critic1": "Qâ‚(s,a) - Twin critic 1",
                "critic2": "Qâ‚‚(s,a) - Twin critic 2", 
                "critic1_target": "QÌ„â‚(s,a) - Target critic 1",
                "critic2_target": "QÌ„â‚‚(s,a) - Target critic 2",
                "enhanced_capacity": [512, 256, 128],
                "total_parameters": (agent.actor.count_params() + 
                                   agent.actor_target.count_params() + 
                                   agent.critic1.count_params() + 
                                   agent.critic2.count_params() +
                                   agent.critic1_target.count_params() +
                                   agent.critic2_target.count_params())
            },
            "td3_hyperparameters": {
                "policy_delay": agent.policy_delay,
                "policy_noise": agent.policy_noise,
                "noise_clip": agent.noise_clip,
                "exploration_noise": agent.exploration_noise,
                "tau": agent.tau
            }
        },
        "ğŸ¢ trading_environment_layer": {
            "enhanced_features": True,
            "features_per_stock": ["momentum_5d", "momentum_20d", "volatility_20d", "rsi", "bollinger_b"],
            "n_stocks": train_env.n_stocks,
            "state_dim": train_env.state_dim,
            "stock_names": train_env.stock_names,
            "testing_days": test_steps
        },
        "ğŸ”„ early_stopping_layer": early_stopping_summary,  # ğŸ†•
        "ğŸ“Š visualization_enhancements": {
            "x_axis_type": "trading_days",
            "charts_use_days": True,
            "day_range": f"1 to {test_steps}",
            "training_episodes": len(episode_data),
            "early_stopped": bool(patience_counter >= patience)
        },
        "ğŸ“ˆ performance_summary": {
            "final_portfolio_value": float(final_value),
            "total_return_pct": float(total_return * 100),
            "max_drawdown_pct": float(max_drawdown * 100),
            "final_cash_ratio": float(final_cash_ratio),
            "testing_days": test_steps,
            "sharpe_ratio": backtest_results['td3_sharpe'],
            "win_rate_pct": backtest_results['win_rate'],
            "enhanced_features_used": True,
            "reward_approach": "active_selection_fusion",
            "algorithm": "TD3",
            "early_stopping_used": True,
            "best_validation_score": float(best_val_score),
            "best_episode": int(best_episode),
            "total_training_episodes": len(episode_data)
        }
    }
    
    # è½‰æ›numpyé¡å‹ä¸¦ä¿å­˜
    system_output = convert_numpy_types(system_output)
    
    with open(f'{exp_dir}/td3_fusion_early_stop_system_output.json', 'w') as f:
        json.dump(system_output, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… TD3 Early Stopping experiment completed!")
    print(f"ğŸ“Š Final Portfolio Value: ${final_value:.2f}")
    print(f"ğŸ“ˆ Total Return: {total_return:.1%}")
    print(f"ğŸ“‰ Max Drawdown: {max_drawdown:.1%}")
    print(f"ğŸ’° Final Cash Ratio: {final_cash_ratio:.1%}")
    print(f"ğŸ¯ Reward Approach: Active Selection Fusion")
    print(f"ğŸ¯ Enhanced State Dim: {train_env.state_dim}")
    print(f"ğŸ“Š Testing Days: {test_steps}")
    print(f"âœ¨ Rich Features: 5 per stock (momentum, volatility, RSI, Bollinger)")
    print(f"ğŸ“Š Sharpe Ratio: {backtest_results['td3_sharpe']:.2f}")
    print(f"ğŸ“Š Win Rate: {backtest_results['win_rate']:.1f}%")
    print(f"ğŸ—ï¸ TD3 Architecture: Actor + Target Actor + Twin Critics + Target Critics")
    print(f"ğŸ¯ Deterministic Policy with Exploration Noise")
    print(f"âœ… Policy Delay: {agent.policy_delay}, Target Smoothing, Twin Critics")
    
    # ğŸ†• Early Stopping çµæœæ‘˜è¦
    print(f"\nğŸ”„ â­ EARLY STOPPING SUMMARY â­")
    print(f"ğŸ† Best Validation Score: {best_val_score:.4f}")
    print(f"ğŸ¯ Best Episode: {best_episode}")
    print(f"ğŸ“Š Total Episodes Trained: {len(episode_data)} / {max_episodes}")
    print(f"ğŸ”„ Early Stopped: {'Yes' if patience_counter >= patience else 'No'}")
    print(f"â³ Final Patience Counter: {patience_counter} / {patience}")
    print(f"ğŸ“ Metric Used: {'Sharpe Ratio' if use_sharpe_ratio else 'Average Reward'}")
    print(f"ğŸ’¾ Best Models Saved: {'Yes' if save_best_models else 'No'}")
    if patience_counter >= patience:
        episodes_saved = max_episodes - len(episode_data)
        print(f"âš¡ Training Efficiency: Saved {episodes_saved} episodes ({episodes_saved/max_episodes*100:.1f}%)")
    
    print(f"\nğŸ”„ Results saved to: {exp_dir}")
    
    print(f"\nğŸ“Š â­ CHARTS WITH TRADING DAYS AS X-AXIS + EARLY STOPPING â­")
    print(f"ğŸ§  enhanced_td3_learning_monitor_early_stopping.png - TD3å­¸ç¿’ç›£æ§ + Early Stopping")
    print(f"ğŸ“ˆ enhanced_td3_weight_evolution_by_days.png")
    print(f"ğŸ“Š enhanced_td3_weight_stacked_by_days.png")
    print(f"ğŸ’° enhanced_td3_stocks_vs_cash_by_days.png")
    print(f"ğŸ“‹ enhanced_td3_comprehensive_metrics.png")
    print(f"ğŸ† enhanced_td3_backtest_comparison_by_days.png")
    print(f"ğŸ¨ professional_stacked_chart.png")
    
    print(f"\nğŸ¯ â­ TD3æ¶æ§‹èªªæ˜ â­")
    print(f"âœ… Actor Network: Ï€(a|s) - ç¢ºå®šæ€§ç­–ç•¥ç¶²çµ¡")
    print(f"âœ… Target Actor: Ï€Ì„(a|s) - ç›®æ¨™ç­–ç•¥ç¶²çµ¡")
    print(f"âœ… Twin Critics: Qâ‚(s,a) & Qâ‚‚(s,a) - é›™é‡åƒ¹å€¼å‡½æ•¸")
    print(f"âœ… Target Critics: QÌ„â‚(s,a) & QÌ„â‚‚(s,a) - ç›®æ¨™åƒ¹å€¼å‡½æ•¸")
    print(f"âŒ No V-Critic Networks - èˆ‡SACä¸åŒ")
    print(f"âŒ No Entropy Term - ç¢ºå®šæ€§ç­–ç•¥")
    print(f"âœ… Policy Delay: æ¯{agent.policy_delay}æ­¥æ›´æ–°ç­–ç•¥")
    print(f"âœ… Target Smoothing: ç›®æ¨™ç­–ç•¥å¹³æ»‘æŠ€è¡“")
    print(f"âœ… Exploration Noise: {agent.exploration_noise} é«˜æ–¯å™ªè²")
    print(f"ğŸ”„ Early Stopping: è‡ªå‹•å„ªåŒ–è¨“ç·´æ•ˆç‡")
    
    print(f"\nğŸ’¾ â­ TD3æ¨¡å‹æ–‡ä»¶ â­")
    print(f"ğŸ¤– best_actor.keras - æœ€ä½³Actorç¶²çµ¡")
    print(f"ğŸ¤– best_critic1.keras - æœ€ä½³Critic1ç¶²çµ¡")
    print(f"ğŸ¤– best_critic2.keras - æœ€ä½³Critic2ç¶²çµ¡")
    print(f"ğŸ”§ td3_*_final.keras - å…¶ä»–æœ€çµ‚ç¶²çµ¡")
    
    print(f"\nğŸ”„ â­ TD3 + Early Stopping ç‰¹æ€§ â­")
    print(f"âœ… ç¢ºå®šæ€§ç­–ç•¥: tanhè¼¸å‡ºï¼Œç„¡éš¨æ©Ÿæ€§")
    print(f"âœ… æ¢ç´¢å™ªè²: è¨“ç·´æ™‚åŠ é«˜æ–¯å™ªè²")
    print(f"âœ… å»¶é²ç­–ç•¥æ›´æ–°: æ¸›å°‘Actoréåº¦æ›´æ–°")
    print(f"âœ… ç›®æ¨™ç­–ç•¥å¹³æ»‘: æ¸›å°‘ç›®æ¨™Qå€¼éä¼°è¨ˆ")
    print(f"âœ… é›™é‡è©•åƒ¹ç¶²çµ¡: Twin Criticså–æœ€å°å€¼")
    print(f"âœ… è»Ÿæ›´æ–°æ‰€æœ‰ç›®æ¨™ç¶²çµ¡: Ï„={agent.tau}")
    print(f"ğŸ”„ è‡ªå‹•åœæ­¢è¨“ç·´: é©—è­‰é›†æŒ‡æ¨™å°å‘")
    print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹ä¿å­˜: ç¢ºä¿æœ€å„ªæ€§èƒ½")
    print(f"âš¡ è¨“ç·´æ•ˆç‡æå‡: é¿å…éåº¦è¨“ç·´")
    
    print(f"\nğŸ’¾ â­ DATA FILES WITH EARLY STOPPING INFO â­")
    print(f"ğŸ“ˆ test_weights_by_days_early_stop.csv - Weights with trading day column")
    print(f"ğŸ“Š validation_scores_history.csv - Validation scores over training")
    print(f"ğŸ”„ early_stopping_summary.json - Early stopping configuration & results")
    print(f"ğŸ§ª enhanced_td3_test_feedback_by_days_early_stop.csv - Test feedback with day info")
    print(f"ğŸ“Š enhanced_td3_backtest_comparison_by_days.csv - Detailed comparison")
    print(f"ğŸ”„ td3_fusion_early_stop_system_output.json - Complete system output")
    
    return exp_dir

def create_enhanced_visualizations_with_days(exp_dir, episode_data, weights_data, training_losses, stock_names, test_steps):
    """ğŸ†• å‰µå»ºä»¥å¤©æ•¸ç‚ºxè»¸çš„å¢å¼·ç‰ˆå¯è¦–åŒ–åœ–è¡¨"""
    plt.style.use('default')
    
    # å‹•æ…‹è³‡ç”¢åˆ—è¡¨
    assets = stock_names + ['CASH']
    n_assets = len(assets)
    
    # æ·±è‰²æ–¹æ¡ˆ
    deep_colors = [
        '#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',
        '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf',
        '#393b79', '#5254a3',
    ]
    cash_color = '#B8860B'
    
    # åˆ†é…é¡è‰²
    colors = []
    for i, asset in enumerate(assets):
        if asset == 'CASH':
            colors.append(cash_color)
        else:
            colors.append(deep_colors[i % len(deep_colors)])
    
    # ğŸ†• 1. æ¸¬è©¦æœŸé–“æ¬Šé‡æ¼”åŒ–ï¼ˆä»¥å¤©æ•¸ç‚ºxè»¸ï¼‰
    plt.figure(figsize=(16, 10))
    
    # ä½¿ç”¨æ¸¬è©¦æœŸé–“çš„å¤©æ•¸ä½œç‚ºxè»¸
    days = list(range(1, len(weights_data) + 1))
    
    for i, asset in enumerate(assets):
        weights = [w[i] for w in weights_data]
        if asset == 'CASH':
            plt.plot(days, weights, label=asset, color=cash_color, 
                    linewidth=3.5, linestyle='--', alpha=0.9)
        else:
            plt.plot(days, weights, label=asset, color=colors[i], 
                    linewidth=2.5, linestyle='-', alpha=0.85)
    
    plt.axhline(y=1/n_assets, color='gray', linestyle=':', alpha=0.5, 
                label=f'Equal Weight ({100/n_assets:.1f}%)')
    plt.xlabel('Trading Day', fontsize=12)
    plt.ylabel('Weight', fontsize=12)
    plt.title(f'Enhanced TD3 Portfolio Evolution with Early Stopping - Testing Period ({test_steps} Days)', 
              fontsize=14, fontweight='bold')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(f'{exp_dir}/plots/enhanced_td3_weight_evolution_by_days.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # ğŸ†• 2. æ¬Šé‡æ¼”åŒ–ç´¯ç©æŸ±ç‹€åœ–ï¼ˆæŒ‰å¤©æ•¸ï¼‰
    plt.figure(figsize=(20, 10))
    
    # æ¡æ¨£é¡¯ç¤ºï¼ˆå¦‚æœå¤©æ•¸å¤ªå¤šï¼‰
    sample_step = max(1, len(weights_data) // 50)
    sample_days = list(range(1, len(weights_data) + 1, sample_step))
    sample_weights = [weights_data[i-1] for i in sample_days]  # èª¿æ•´ç´¢å¼•
    
    weights_by_asset = []
    for i in range(n_assets):
        asset_weights = [w[i] for w in sample_weights]
        weights_by_asset.append(asset_weights)
    
    bottom_values = np.zeros(len(sample_days))
    
    for i, asset in enumerate(assets):
        if asset == 'CASH':
            plt.bar(sample_days, weights_by_asset[i], 
                    bottom=bottom_values, label=asset, color=cash_color, 
                    alpha=0.8, edgecolor='white', linewidth=1.5)
        else:
            plt.bar(sample_days, weights_by_asset[i], 
                    bottom=bottom_values, label=asset, color=colors[i], 
                    alpha=0.85, edgecolor='white', linewidth=0.5)
        bottom_values += np.array(weights_by_asset[i])
    
    plt.xlabel('Trading Day', fontsize=12)
    plt.ylabel('Weight', fontsize=12)
    plt.title(f'Enhanced TD3 Portfolio Weight Evolution with Early Stopping - By Trading Days', 
              fontsize=14, fontweight='bold')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
    plt.grid(True, alpha=0.3, axis='y')
    plt.tight_layout()
    plt.savefig(f'{exp_dir}/plots/enhanced_td3_weight_stacked_by_days.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # ğŸ†• 3. è‚¡ç¥¨vsç¾é‡‘å°æ¯”åœ–ï¼ˆæŒ‰å¤©æ•¸ï¼‰
    plt.figure(figsize=(15, 8))
    
    stock_weights = [sum(w[:-1]) for w in weights_data]
    cash_weights = [w[-1] for w in weights_data]
    
    plt.plot(days, stock_weights, label=f'Total Stocks ({len(stock_names)})', 
             color='#C41E3A', linewidth=3.5, alpha=0.9)
    plt.plot(days, cash_weights, label='Cash', 
             color=cash_color, linewidth=3.5, linestyle='--', alpha=0.9)
    
    plt.axhline(y=0.5, color='#2F4F4F', linestyle=':', alpha=0.8, linewidth=2, label='50% Line')
    plt.axhline(y=0.8, color='#191970', linestyle=':', alpha=0.8, linewidth=2, label='Benchmark Stocks (80%)')
    plt.axhline(y=0.2, color='#FF8C00', linestyle=':', alpha=0.8, linewidth=2, label='Benchmark Cash (20%)')
    
    plt.xlabel('Trading Day', fontsize=12)
    plt.ylabel('Weight', fontsize=12)
    plt.title('Enhanced TD3 Stocks vs Cash Allocation with Early Stopping Over Trading Days', fontsize=14, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(f'{exp_dir}/plots/enhanced_td3_stocks_vs_cash_by_days.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # ğŸ†• 4. ç¶œåˆæŒ‡æ¨™åœ–ï¼ˆä¿®å¾©xè»¸ï¼‰
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Enhanced TD3 Comprehensive Metrics with Early Stopping', fontsize=16, fontweight='bold')
    
    # æŠ•è³‡çµ„åˆåƒ¹å€¼å¢é•·ï¼ˆè¨“ç·´æœŸé–“ - ä¿æŒepisodeï¼‰
    episodes_list = [data['episode'] for data in episode_data]
    portfolio_values = [data['portfolio_value'] for data in episode_data]
    
    axes[0,0].plot(episodes_list, [(val/10000-1)*100 for val in portfolio_values], 'g-', linewidth=2, label='Enhanced TD3')
    axes[0,0].set_title('Portfolio Value Growth During Training (%)', fontweight='bold')
    axes[0,0].set_xlabel('Training Episode')
    axes[0,0].set_ylabel('Return (%)')
    axes[0,0].grid(True, alpha=0.3)
    axes[0,0].legend()
    
    # Episodeçå‹µï¼ˆè¨“ç·´æœŸé–“ - ä¿æŒepisodeï¼‰
    returns = [data['return'] for data in episode_data]
    axes[0,1].plot(episodes_list, returns, 'b-', linewidth=2)
    axes[0,1].set_title('Training Episode Rewards (Fusion Version + Early Stopping)', fontweight='bold')
    axes[0,1].set_xlabel('Training Episode')
    axes[0,1].set_ylabel('Reward')
    axes[0,1].grid(True, alpha=0.3)
    
    # ç¾é‡‘æ¯”ä¾‹ï¼ˆğŸ†• æ¸¬è©¦æœŸé–“ - æ”¹ç‚ºå¤©æ•¸ï¼‰
    cash_ratios_test = [w[-1]*100 for w in weights_data]
    axes[1,0].plot(days, cash_ratios_test, 'orange', linewidth=2, label='Enhanced TD3')
    axes[1,0].axhline(y=20, color='red', linestyle='--', alpha=0.7, label='Benchmark (20%)')
    axes[1,0].set_title('Cash Allocation During Testing (%)', fontweight='bold')
    axes[1,0].set_xlabel('Trading Day')  # ğŸ†• æ”¹ç‚ºTrading Day
    axes[1,0].set_ylabel('Cash %')
    axes[1,0].grid(True, alpha=0.3)
    axes[1,0].legend()
    
    # TD3è¨“ç·´æå¤±ï¼ˆè¨“ç·´æŒ‡æ¨™ - ä¿æŒupdate stepsï¼‰
    if training_losses:
        actor_losses = [loss['actor_loss'] for loss in training_losses]
        update_steps = list(range(1, len(training_losses) + 1))
        
        axes[1,1].plot(update_steps, actor_losses, 'r-', linewidth=2, label='Actor Loss')
        
        axes[1,1].set_title('Enhanced TD3 Training Progress with Early Stopping', fontweight='bold')
        axes[1,1].set_xlabel('Training Update Step')
        axes[1,1].set_ylabel('Actor Loss', color='r')
        axes[1,1].tick_params(axis='y', labelcolor='r')
        axes[1,1].legend()
        axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{exp_dir}/plots/enhanced_td3_comprehensive_metrics.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… Enhanced TD3 visualizations with Early Stopping and trading days created!")

def create_detailed_backtest_comparison_by_days(exp_dir, test_env, stock_names):
    """ğŸ†• ç”Ÿæˆä»¥å¤©æ•¸ç‚ºxè»¸çš„è©³ç´°å›æ¸¬å°æ¯”"""
    
    print("ğŸ“Š Generating detailed backtest comparison by trading days with Early Stopping...")
    
    # è¨ˆç®—åŸºæº–æŒ‡æ¨™
    benchmark_values = test_env.calculate_equal_weight_benchmark()
    td3_values = test_env.portfolio_values_history
    
    min_length = min(len(benchmark_values), len(td3_values))
    benchmark_values = benchmark_values[:min_length]
    td3_values = td3_values[:min_length]
    
    # è¨ˆç®—æ”¶ç›Šç‡åºåˆ—
    td3_returns = [(td3_values[i] / td3_values[i-1] - 1) for i in range(1, len(td3_values))]
    benchmark_returns = [(benchmark_values[i] / benchmark_values[i-1] - 1) for i in range(1, len(benchmark_values))]
    
    # è¨ˆç®—é—œéµæŒ‡æ¨™
    td3_total_return = (td3_values[-1] / 10000.0 - 1) * 100
    benchmark_total_return = (benchmark_values[-1] / 10000.0 - 1) * 100
    
    # è¨ˆç®—æœ€å¤§å›æ’¤
    def calculate_max_drawdown(values):
        peak_values = np.maximum.accumulate(values)
        drawdowns = (np.array(values) - peak_values) / peak_values
        return abs(np.min(drawdowns)) * 100
    
    td3_max_drawdown = calculate_max_drawdown(td3_values)
    benchmark_max_drawdown = calculate_max_drawdown(benchmark_values)
    
    # è¨ˆç®—Sharpe Ratio
    risk_free_rate = test_env.daily_cash_return * 252
    
    def calculate_sharpe(returns, risk_free_rate):
        if len(returns) == 0:
            return 0
        excess_returns = np.array(returns) - risk_free_rate/252
        return np.mean(excess_returns) / (np.std(excess_returns) + 1e-8) * np.sqrt(252)
    
    td3_sharpe = calculate_sharpe(td3_returns, risk_free_rate)
    benchmark_sharpe = calculate_sharpe(benchmark_returns, risk_free_rate)
    
    # å…¶ä»–æŒ‡æ¨™
    td3_final_cash = test_env.weights[-1] * 100
    td3_volatility = np.std(td3_returns) * np.sqrt(252) * 100 if len(td3_returns) > 0 else 0
    benchmark_volatility = np.std(benchmark_returns) * np.sqrt(252) * 100 if len(benchmark_returns) > 0 else 0
    
    # å‹ç‡
    if len(td3_returns) == len(benchmark_returns):
        win_days = sum(1 for i in range(len(td3_returns)) if td3_returns[i] > benchmark_returns[i])
        win_rate = (win_days / len(td3_returns)) * 100
    else:
        win_rate = 0
    
    # ç”Ÿæˆå°æ¯”è¡¨æ ¼
    comparison_data = {
        'Metric': [
            'Total Return (%)',
            'Max Drawdown (%)', 
            'Sharpe Ratio',
            'Volatility (%)',
            'Final Cash (%)',
            'Win Rate (%)',
            'Trading Days',
            'Final Portfolio Value ($)',
            'Early Stopping Used'
        ],
        'Enhanced TD3': [
            f"{td3_total_return:.1f}%",
            f"{td3_max_drawdown:.1f}%",
            f"{td3_sharpe:.2f}",
            f"{td3_volatility:.1f}%",
            f"{td3_final_cash:.1f}%",
            f"{win_rate:.1f}%",
            f"{len(td3_values)-1}",
            f"${td3_values[-1]:,.0f}",
            "YES âœ…"
        ],
        'Equal Weight Benchmark': [
            f"{benchmark_total_return:.1f}%",
            f"{benchmark_max_drawdown:.1f}%", 
            f"{benchmark_sharpe:.2f}",
            f"{benchmark_volatility:.1f}%",
            "20.0%",
            "50.0%",
            f"{len(benchmark_values)-1}",
            f"${benchmark_values[-1]:,.0f}",
            "N/A"
        ]
    }
    
    # ä¿å­˜ç‚ºCSV
    comparison_df = pd.DataFrame(comparison_data)
    comparison_df.to_csv(f'{exp_dir}/data/enhanced_td3_backtest_comparison_by_days.csv', index=False)
    
    # ğŸ†• ç”Ÿæˆä»¥å¤©æ•¸ç‚ºxè»¸çš„å¯è¦–åŒ–å°æ¯”åœ–è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('ğŸ“Š Enhanced TD3 Backtest Performance with Early Stopping (By Trading Days)', fontsize=16, fontweight='bold')
    
    # 1. ç´¯ç©æ”¶ç›Šæ›²ç·šï¼ˆæŒ‰å¤©æ•¸ï¼‰
    days = range(1, len(td3_values) + 1)
    td3_cumulative = [(val / 10000 - 1) * 100 for val in td3_values]
    benchmark_cumulative = [(val / 10000 - 1) * 100 for val in benchmark_values]
    
    axes[0,0].plot(days, td3_cumulative, label='Enhanced TD3 (Early Stop)', color='#2E8B57', linewidth=2.5)
    axes[0,0].plot(days, benchmark_cumulative, label='Benchmark', color='#4682B4', linewidth=2.5, linestyle='--')
    axes[0,0].set_title('Cumulative Return Comparison')
    axes[0,0].set_xlabel('Trading Day')
    axes[0,0].set_ylabel('Cumulative Return (%)')
    axes[0,0].legend()
    axes[0,0].grid(True, alpha=0.3)
    
    # 2. æ—¥æ”¶ç›Šç‡å°æ¯”
    if len(td3_returns) > 0 and len(benchmark_returns) > 0:
        return_days = range(2, len(td3_values) + 1)  # å¾ç¬¬2å¤©é–‹å§‹
        axes[0,1].plot(return_days, [r*100 for r in td3_returns], label='Enhanced TD3 (Early Stop)', color='#2E8B57', alpha=0.7, linewidth=1)
        axes[0,1].plot(return_days, [r*100 for r in benchmark_returns], label='Benchmark', color='#4682B4', alpha=0.7, linewidth=1)
        axes[0,1].set_title('Daily Returns Comparison')
        axes[0,1].set_xlabel('Trading Day')
        axes[0,1].set_ylabel('Daily Return (%)')
        axes[0,1].legend()
        axes[0,1].grid(True, alpha=0.3)
    
    # 3. ç¾é‡‘é…ç½®å°æ¯”ï¼ˆæŒ‰å¤©æ•¸ï¼‰
    if hasattr(test_env, 'weights_history') and len(test_env.weights_history) > 0:
        cash_ratios = [w[-1] * 100 for w in test_env.weights_history]
        cash_days = range(1, len(cash_ratios) + 1)
        
        axes[1,0].plot(cash_days, cash_ratios, label='Enhanced TD3 Cash % (Early Stop)', color='#B8860B', linewidth=2.5)
        axes[1,0].axhline(y=20, color='#4682B4', linestyle='--', linewidth=2.5, label='Benchmark Cash (20%)')
        axes[1,0].set_title('Cash Allocation Over Trading Days')
        axes[1,0].set_xlabel('Trading Day')
        axes[1,0].set_ylabel('Cash Allocation (%)')
        axes[1,0].legend()
        axes[1,0].grid(True, alpha=0.3)
    
    # 4. æ»¾å‹•Sharpe Ratioå°æ¯”
    window = 30  # 30å¤©æ»¾å‹•çª—å£
    if len(td3_returns) >= window:
        rolling_days = range(window, len(td3_returns) + 1)
        td3_rolling_sharpe = []
        benchmark_rolling_sharpe = []
        
        for i in range(window, len(td3_returns) + 1):
            td3_window_returns = td3_returns[i-window:i]
            benchmark_window_returns = benchmark_returns[i-window:i]
            
            td3_rs = calculate_sharpe(td3_window_returns, risk_free_rate)
            benchmark_rs = calculate_sharpe(benchmark_window_returns, risk_free_rate)
            
            td3_rolling_sharpe.append(td3_rs)
            benchmark_rolling_sharpe.append(benchmark_rs)
        
        axes[1,1].plot(rolling_days, td3_rolling_sharpe, label='Enhanced TD3 (Early Stop)', color='#2E8B57', linewidth=2)
        axes[1,1].plot(rolling_days, benchmark_rolling_sharpe, label='Benchmark', color='#4682B4', linewidth=2, linestyle='--')
        axes[1,1].set_title(f'Rolling Sharpe Ratio ({window}-Day Window)')
        axes[1,1].set_xlabel('Trading Day')
        axes[1,1].set_ylabel('Sharpe Ratio')
        axes[1,1].legend()
        axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{exp_dir}/plots/enhanced_td3_backtest_comparison_by_days.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # æ‰“å°å°æ¯”è¡¨æ ¼
    print("\n" + "="*80)
    print("ğŸ“Š ENHANCED TD3 BACKTEST COMPARISON WITH EARLY STOPPING (BY TRADING DAYS)")
    print("="*80)
    
    print(f"{'Metric':<25} {'Enhanced TD3':<15} {'Benchmark':<20}")
    print("-" * 80)
    
    for i, metric in enumerate(comparison_data['Metric']):
        our = comparison_data['Enhanced TD3'][i]
        bench = comparison_data['Equal Weight Benchmark'][i]
        print(f"{metric:<25} {our:<15} {bench:<20}")
    
    print("="*80)
    
    return {
        'td3_total_return': td3_total_return,
        'benchmark_total_return': benchmark_total_return,
        'td3_sharpe': td3_sharpe,
        'benchmark_sharpe': benchmark_sharpe,
        'win_rate': win_rate,
        'trading_days': len(td3_values) - 1,
        'comparison_df': comparison_df
    }

def create_professional_stacked_chart_final(exp_dir, weights_data, stock_names, dates):
    """
    å‰µå»ºå°ˆæ¥­çš„å †ç–Šé¢ç©åœ– - å®Œå…¨åŒ¹é…ä½ çš„åƒè€ƒåœ–ç‰‡é¢¨æ ¼ï¼ŒåŠ ä¸ŠEarly Stoppingæ¨™è¨˜
    """
    print("ğŸ¨ Generating professional stacked chart with Early Stopping (final version)...")
    
    try:
        # 1. æ•¸æ“šæº–å‚™å’Œé©—è­‰
        assets = stock_names + ['Cash']  # æ³¨æ„é€™è£¡ç”¨ 'Cash' è€Œä¸æ˜¯ 'CASH'
        weights_array = np.array(weights_data)
        
        print(f"ğŸ“Š åŸå§‹æ•¸æ“šå½¢ç‹€: {weights_array.shape}")
        print(f"ğŸ“Š è³‡ç”¢åˆ—è¡¨: {assets}")
        
        # ç¢ºä¿æ˜¯2Dæ•¸çµ„
        if weights_array.ndim == 1:
            weights_array = weights_array.reshape(1, -1)
        
        # æª¢æŸ¥ä¸¦ä¿®å¾©ç¶­åº¦ä¸åŒ¹é…å•é¡Œ
        if weights_array.shape[1] != len(assets):
            print(f"âš ï¸ ç¶­åº¦ä¸åŒ¹é…: weightsæœ‰{weights_array.shape[1]}åˆ—ï¼Œä½†éœ€è¦{len(assets)}åˆ—")
            if weights_array.shape[1] == len(assets) - 1:
                # å¦‚æœç¼ºå°‘ç¾é‡‘åˆ—ï¼Œæ·»åŠ å®ƒ
                cash_weights = 1 - np.sum(weights_array, axis=1, keepdims=True)
                weights_array = np.column_stack([weights_array, cash_weights])
                print("âœ… è‡ªå‹•æ·»åŠ äº†ç¾é‡‘åˆ—")
            else:
                # èª¿æ•´è³‡ç”¢åˆ—è¡¨åŒ¹é…æ•¸æ“š
                assets = assets[:weights_array.shape[1]]
                print(f"âœ… èª¿æ•´è³‡ç”¢åˆ—è¡¨ç‚º: {assets}")
        
        # å‰µå»ºDataFrame
        df = pd.DataFrame(weights_array, columns=assets)
        
        # è™•ç†æ—¥æœŸç´¢å¼•
        if isinstance(dates, (pd.DatetimeIndex, list, np.ndarray)):
            try:
                if len(dates) >= len(df):
                    df.index = pd.to_datetime(dates[:len(df)])
                else:
                    # å¦‚æœæ—¥æœŸä¸å¤ ï¼Œç”Ÿæˆé»˜èªæ—¥æœŸ
                    df.index = pd.date_range('2019-03-14', periods=len(df), freq='D')
                    print("âš ï¸ æ—¥æœŸæ•¸é‡ä¸è¶³ï¼Œä½¿ç”¨é»˜èªæ—¥æœŸç¯„åœ")
            except:
                # å¦‚æœæ—¥æœŸè½‰æ›å¤±æ•—ï¼Œä½¿ç”¨æ­¥æ•¸ä½œç‚ºç´¢å¼•
                df.index = range(len(df))
                print("âš ï¸ æ—¥æœŸè½‰æ›å¤±æ•—ï¼Œä½¿ç”¨æ­¥æ•¸ç´¢å¼•")
        else:
            df.index = range(len(df))
            print("âš ï¸ ä½¿ç”¨æ­¥æ•¸ç´¢å¼•")
        
        print(f"âœ… DataFrameå‰µå»ºæˆåŠŸ: {df.shape}")
        
        # 2. å¯é¸çš„é‡æ¡æ¨£
        if len(df) > 200:
            original_len = len(df)
            df = df.resample('W').first().dropna() if hasattr(df.index, 'freq') else df.iloc[::7]
            print(f"ğŸ“… é‡æ¡æ¨£: {original_len} -> {len(df)} æ•¸æ“šé»")
        
        # 3. å‰µå»ºåœ–è¡¨ - ä½¿ç”¨èˆ‡ä½ åƒè€ƒåœ–ç‰‡ç›¸ä¼¼çš„é¢¨æ ¼
        fig, ax = plt.subplots(figsize=(12, 6))  # èˆ‡åƒè€ƒåœ–ç‰‡ç›¸ä¼¼çš„æ¯”ä¾‹
        
        # è¨­ç½®é¡è‰² - ä½¿ç”¨æŸ”å’Œçš„é¡è‰²ï¼Œé¡ä¼¼åƒè€ƒåœ–ç‰‡
        colors = [
            '#FF9999',  # æ·ºç´…/ç²‰è‰² (é¡ä¼¼ä½ åœ–ç‰‡ä¸­çš„é¡è‰²)
            '#66B2FF',  # æ·ºè—è‰²
            '#FFB366',  # æ·ºæ©™è‰²  
            '#FF66FF',  # æ·ºç´«è‰²
            '#66FFB2',  # æ·ºç¶ è‰²
            '#B366FF',  # è—ç´«è‰²
            '#FFFF66',  # æ·ºé»ƒè‰²
            '#66FFFF'   # æ·ºé’è‰²
        ]
        
        # ä½¿ç”¨stackplotå‰µå»ºå †ç–Šé¢ç©åœ–
        y_data = [df[asset].values for asset in assets]
        
        # å¦‚æœç´¢å¼•æ˜¯æ•¸å€¼å‹ï¼ˆæ­¥æ•¸ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
        if isinstance(df.index[0], (int, np.integer)):
            x_data = df.index
        else:
            x_data = df.index
        
        stack = ax.stackplot(x_data, *y_data, 
                           labels=assets,
                           colors=colors[:len(assets)],
                           alpha=0.8)
        
        # 4. æ ¼å¼åŒ–åœ–è¡¨ - åŒ¹é…åƒè€ƒåœ–ç‰‡é¢¨æ ¼
        
        # Yè»¸æ ¼å¼åŒ–ç‚ºæ¬Šé‡ï¼ˆ0-1ï¼‰
        ax.set_ylim(0, 1)
        ax.set_ylabel('Weights', fontsize=12)
        
        # å¦‚æœä½¿ç”¨æ—¥æœŸç´¢å¼•
        if hasattr(df.index, 'year'):
            # æ ¼å¼åŒ–Xè»¸æ—¥æœŸ
            if len(df) > 50:
                ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
                ax.xaxis.set_major_locator(mdates.MonthLocator(interval=max(1, len(df)//10)))
            else:
                ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
            
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)
            
            # è¨­ç½®æ¨™é¡Œ - é¡ä¼¼åƒè€ƒåœ–ç‰‡ï¼ŒåŠ ä¸ŠEarly Stoppingæ¨™è¨˜
            start_date = df.index.min().strftime('%Y-%m-%d')
            end_date = df.index.max().strftime('%Y-%m-%d')
            ax.set_title(f'TD3 + Early Stopping - Portfolio Weights - OOS {start_date} to {end_date}', 
                        fontsize=14, color='gray')
        else:
            # ä½¿ç”¨æ­¥æ•¸ç´¢å¼•
            ax.set_xlabel('Time Steps')
            ax.set_title('Portfolio Weights Over Time (TD3 + Early Stopping)', fontsize=14, color='gray')
        
        # åœ–ä¾‹è¨­ç½® - æ”¾åœ¨å³å´ï¼Œé¡ä¼¼åƒè€ƒåœ–ç‰‡
        legend = ax.legend(title='Stock', loc='center left', bbox_to_anchor=(1, 0.5), 
                          frameon=True, fancybox=True, shadow=False, fontsize=10)
        legend.get_title().set_fontsize(12)
        
        # ç§»é™¤é ‚éƒ¨å’Œå³å´é‚Šæ¡†
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['left'].set_color('lightgray')
        ax.spines['bottom'].set_color('lightgray')
        
        # è¨­ç½®ç¶²æ ¼ - æ·ºè‰²ï¼Œé¡ä¼¼åƒè€ƒåœ–ç‰‡
        ax.grid(True, alpha=0.3, color='lightgray', linestyle='-', linewidth=0.5)
        ax.set_axisbelow(True)
        
        # èª¿æ•´å¸ƒå±€
        plt.tight_layout()
        plt.subplots_adjust(right=0.85)  # ç‚ºå³å´åœ–ä¾‹ç•™ç©ºé–“
        
        # ä¿å­˜åœ–è¡¨
        os.makedirs(f'{exp_dir}/plots', exist_ok=True)
        plt.savefig(f'{exp_dir}/plots/professional_stacked_chart.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print("âœ… å°ˆæ¥­å †ç–Šé¢ç©åœ–å‰µå»ºæˆåŠŸï¼ˆå«Early Stoppingï¼‰ï¼")
        print(f"ğŸ’¾ ä¿å­˜ä½ç½®: {exp_dir}/plots/professional_stacked_chart.png")
        
        return {
            'success': True,
            'data_points': len(df),
            'assets': assets,
            'date_range': f"{df.index.min()} to {df.index.max()}" if hasattr(df.index, 'year') else f"0 to {len(df)}"
        }
        
    except Exception as e:
        print(f"âŒ å‰µå»ºåœ–è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return {'success': False, 'error': str(e)}

if __name__ == "__main__":
    # é‹è¡ŒTD3å¯¦é©—ï¼Œé›†æˆEarly Stoppingæ©Ÿåˆ¶
    print("ğŸš€ Running TD3 Twin Delayed DDPG Experiment with Early Stopping...")
    exp_dir = run_td3_fusion_experiment_with_early_stopping("nine_stock_prices.csv")
    print(f"\nğŸ‰ TD3 Early Stopping experiment completed: {exp_dir}")
    print(f"ğŸ“Š Twin Delayed DDPG with deterministic policy and Early Stopping!")
    print(f"ğŸ“Š All charts now use Trading Days (1, 2, 3, ..., N) as x-axis!")
    print(f"ğŸ—ï¸ TD3: Actor + Target Actor + Twin Critics + Target Critics")
    print(f"ğŸ¯ Deterministic Policy + Exploration Noise + Policy Delay + Target Smoothing")
    print(f"ğŸ”„ Early Stopping: è‡ªå‹•å„ªåŒ–è¨“ç·´ï¼Œé¿å…éåº¦è¨“ç·´")
    print(f"ğŸ’¾ Best Model Selection: è¼‰å…¥æœ€ä½³é©—è­‰æ€§èƒ½æ¨¡å‹é€²è¡Œæ¸¬è©¦")